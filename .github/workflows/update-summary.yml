name: Daily News Summary

on:
  schedule:
    - cron: "0 7 * * *"   # every day at 07:00 UTC
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          ref: gh-pages

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - run: pip install requests pytz

      - name: Fetch summaries for Astro & AI
        env:
          PROXY_URL: ${{ secrets.PROXY_URL }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          PROXY_KEY: ${{ secrets.PROXY_KEY }}
        run: |
          python - <<'PY'
          import os, json, requests, datetime
          from pytz import timezone

          PROXY_URL   = os.environ["PROXY_URL"].rstrip("/")
          API_KEY     = os.environ["OPENAI_API_KEY"]
          HF_TOKEN    = os.environ.get("HF_TOKEN","")
          PROXY_KEY   = os.environ.get("PROXY_KEY","")

          headers = {"Content-Type": "application/json"}
          if HF_TOKEN:
              headers["Authorization"] = f"Bearer {HF_TOKEN}"
          if PROXY_KEY:
              headers["x-proxy-key"] = PROXY_KEY

          def fetch_and_save(subject, audience, tag):
              payload = {
                  "api_key": API_KEY,
                  "subject_area": subject,
                  "content_type": "news",
                  "audience": audience,
                  "days_limit": 1,
                  "top_entries": 10
              }

              print(f"Fetching {subject} summary for {audience} ...")
              r = requests.post(f"{PROXY_URL}/api/summarize", json=payload, headers=headers, timeout=180)
              print("Status:", r.status_code)
              if r.status_code != 200:
                  print(r.text)
                  r.raise_for_status()
              data = r.json()

              os.makedirs("data", exist_ok=True)
              now = datetime.datetime.now(timezone("UTC"))
              ymd = now.strftime("%Y-%m-%d")

              # Save JSON
              with open(f"data/{tag}_latest.json","w",encoding="utf-8") as f:
                  json.dump(data, f, ensure_ascii=False, indent=2)
              with open(f"data/{tag}_{ymd}.json","w",encoding="utf-8") as f:
                  json.dump(data, f, ensure_ascii=False, indent=2)

              # Save HTML
              html = [f"<section><h2>ðŸ§  {subject.title()} Daily Summary</h2>"]
              html.append(f"<p>{data.get('bulk_summary','')}</p>")
              html.append("<h3>Entries</h3><ol>")
              for e in data.get("entries",[]):
                  t = e.get("title","").replace("<","&lt;")
                  l = e.get("link","#")
                  p = e.get("published","")
                  s = e.get("summary","").replace("<","&lt;")
                  html.append(f"<li><a href='{l}' target='_blank'>{t}</a><br><small>{p}</small><br>{s}</li>")
              html.append("</ol></section>")
              with open(f"data/{tag}_latest.html","w",encoding="utf-8") as f:
                  f.write("\n".join(html))

          # Fetch both
          fetch_and_save("astro", "astro_enthusiasts", "astro")
          fetch_and_save("ai", "ai_enthusiasts", "ai")

          PY

      - name: Commit & push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          date > data/last_run.txt
          git add data/*.json data/*.html data/last_run.txt
          git commit -m "daily update $(date)" || echo "no changes"
          git push origin gh-pages
