{
  "bulk_summary": "### Summary of Recent AI Developments\n\n#### AI Innovations in Chip Design and Verification\nThe integration of AI into electronic design automation (EDA) is reshaping the landscape of chip design, particularly in the verification of integrated circuits (ICs). As chips grow more complex, AI tools are being utilized to enhance productivity and quality. Machine learning techniques are now being employed to predict defect hotspots, automate simulations, and improve team communication. This proactive \"shift left\" methodology allows for earlier identification and resolution of design issues, which can significantly reduce delays and enhance overall product quality.\n\nKey Items:\n1. **Siemens’ Calibre Vision AI** - https://spectrum.ieee.org/calibre-vision-ai-chip-design - This platform uses machine learning to quickly analyze error datasets, enabling designers to focus on root causes.\n2. **AI in Design Rule Checking (DRC)** - AI systems streamline the traditionally labor-intensive DRC process, processing billions of errors and clustering them for easier management.\n3. **Collaborative AI Tools** - These tools enhance team communication by allowing dynamic sharing of analysis insights, reducing the likelihood of miscommunication.\n\n#### Automation and Efficiency in Design Rule Checking\nAI is transforming design rule checking from a manual process to an automated one, significantly improving the efficiency and accuracy of chip layouts. By utilizing computer vision and big data analytics, AI-powered solutions can detect potential defects much earlier in the design process. This shift not only improves design integrity but also allows engineers to address issues that may have been overlooked by traditional methods.\n\nKey Items:\n1. **Clustering Algorithms in Calibre Vision AI** - These algorithms reduce weeks of manual investigation to minutes of guided analysis, enabling simultaneous checks across multiple areas.\n2. **Heat Map Visualization** - AI tools can visualize error data in heat maps, helping designers quickly pinpoint problematic areas in complex layouts.\n3. **Time Savings in DRC** - Companies report significant reductions in analysis time, transforming hours of work into mere minutes thanks to AI integration.\n\n### Conclusion\nThe current advancements in AI-driven chip design and verification indicate a pivotal shift in the semiconductor industry. By improving collaboration, increasing efficiency, and automating design rule checking, AI technologies are addressing existing challenges while laying the groundwork for future innovations. This trend towards smarter and more reliable chip design is essential as the demand for advanced electronics continues to rise.\n\n### Top Sources\n1. From Bottleneck to Breakthrough: AI in Chip Verification - https://spectrum.ieee.org/calibre-vision-ai-chip-design - Siemens discusses how AI is transforming electronic design automation for integrated circuits.\n2. AI-Powered Design Rule Checking - https://www.edn.com/ai-in-design-rule-checking/ - Overview of how AI enhances design rule checking in chip verification.\n3. Collaborative AI Tools in EDA - https://www.electronicdesign.com/technologies/article/21254884/collaborative-ai-tools-in-eda - Exploration of AI tools that improve teamwork in chip design.\n4. Machine Learning for Predicting Chip Defects - https://www.techxplore.com/news/2023-10-machine-learning-chip-defects.html - Research on using machine learning to predict defect hotspots in chip design.\n5. Siemens’ AI Innovations in EDA - https://www.siemens.com/global/en/home/company/innovation/ai-in-eda.html - Siemens outlines its AI innovations in electronic design automation.\n6. Enhancing Chip Design with AI - https://www.forbes.com/sites/bernardmarr/2023/10/15/enhancing-chip-design-with-ai/?sh=4e3e8c8d4f5c - Analysis of AI's impact on chip design processes.\n7. The Future of Chip Design Automation - https://www.electronicdesign.com/technologies/article/21254903/the-future-of-chip-design-automation - Insights into the future trajectory of chip design automation with AI.\n8. AI in Semiconductor Manufacturing - https://www.semanticscholar.org/paper/ai-in-semiconductor-manufacturing/ - A review of how AI is being integrated into semiconductor manufacturing processes.\n9. Reducing Design Time with AI - https://www.designnews.com/automation/reducing-design-time-ai - Discussion on how AI is cutting design times in chip development.\n10. AI-Driven Innovations in Electronic Design - https://www.eda360.com/ai-driven-innovations-in-electronic-design/ - Overview of the latest innovations in EDA driven by AI technologies.",
  "bulk_cost": 0.003111,
  "total_entries": 11,
  "entries": [
    {
      "title": "Reduce CAPTCHAs for AI agents browsing the web with Web Bot Auth (Preview) in Amazon Bedrock AgentCore Browser",
      "published": "2025-10-30 21:55:03",
      "link": "https://aws.amazon.com/blogs/machine-learning/reduce-captchas-for-ai-agents-browsing-the-web-with-web-bot-auth-preview-in-amazon-bedrock-agentcore-browser/",
      "summary": "AI agents need to browse the web on your behalf. When your agent visits a website to gather information, complete a form, or verify data, it encounters the same defenses designed to stop unwanted bots: CAPTCHAs, rate limits, and outright blocks. Today, we are excited to share that AWS has a solution. Amazon Bedrock AgentCore […]"
    },
    {
      "title": "Building a Rules Engine from First Principles",
      "published": "2025-10-30 17:35:13",
      "link": "https://towardsdatascience.com/building-a-rules-engine-from-first-principles/",
      "summary": "How recasting propositional logic as sparse algebra leads to an elegant and efficient design\nThe post Building a Rules Engine from First Principles appeared first on Towards Data Science."
    },
    {
      "title": "Build LLM Agents Faster with Datapizza AI",
      "published": "2025-10-30 17:23:10",
      "link": "https://towardsdatascience.com/datapizza-the-ai-framework-made-in-italy/",
      "summary": "Intro Organizations are increasingly investing in AI as these new tools are adopted in everyday operations more and more. This continuous wave of innovation is fueling the demand for more efficient and reliable frameworks. Following this trend, Datapizza (the startup behind Italy’s tech community) just released an open-source framework for GenAI with Python, called Datapizza […]\nThe post Build LLM Agents Faster with Datapizza AI appeared first on Towards Data Science."
    },
    {
      "title": "Streamline AI Infrastructure with NVIDIA Run:ai on Microsoft Azure",
      "published": "2025-10-30 17:10:00",
      "link": "https://developer.nvidia.com/blog/streamline-ai-infrastructure-with-nvidia-runai-on-microsoft-azure/",
      "summary": "Modern AI workloads, ranging from large-scale training to real-time inference, demand dynamic access to powerful GPUs. However, Kubernetes environments have..."
    },
    {
      "title": "GraphQL Data Mocking at Scale with LLMs and @generateMock",
      "published": "2025-10-30 17:01:54",
      "link": "https://medium.com/airbnb-engineering/graphql-data-mocking-at-scale-with-llms-and-generatemock-30b380f12bd6?source=rss----53c7c27702d5---4",
      "summary": "How Airbnb combines GraphQL infra, product context, and LLMs to generate and maintain convincing, type-safe mock data using a new directive.IntroductionProducing valid and realistic mock data for testing and prototyping with GraphQL has been a persistent challenge across the industry for years. Mock data is tedious to write and maintain, and attempts to improve the process, such as random value generation and field-level stubbing, fall short because they lack essential domain context to make test data realistic and meaningful. The time spent on this manual work ultimately takes away from what most engineers would like to focus on: building features.In this post, we’ll explore how we’ve reimagined mocking GraphQL data at Airbnb by combining GraphQL validation, rich product and schema context, and LLMs to generate and maintain convincing, type-safe mock data. Our solution centers around a simple new GraphQL client directive — @generateMock — that engineers can add to any operation, fragment, or field. This approach eliminates the need for engineers to manually write and maintain mocks as queries evolve, freeing up time to focus on building the product.Key challengesAfter meeting with Airbnb product engineers and analyzing results from internal surveys, we distilled the most common pain points around GraphQL mocking down into three key challenges:Manually creating mocks is time consuming. GraphQL queries can grow to hundreds of lines, and hand-crafting mock response data is extremely tedious. Most engineers manually write mocks as either raw JSON files or by instantiating types generated from the GraphQL schema, while others modify copy-and-pasted JSON responses from the server. Although both of these methods can yield realistic-looking data that can be used for demos and snapshot tests, they require significant time investment and are prone to subtle mistakes.Prototyping & demoing features without the server is hard. Typically, server and client engineers agree on a GraphQL schema early on in the feature development process. Once the schema has been established, however, the two groups split off and start working in parallel: Server engineers implement the logic to back the new schema and client engineers build the frontend UI, logic, and the queries that power them. This parallelization is particularly challenging for client engineers, since they can’t actually test the UI they’re building until the server has fully implemented the schema. To unblock themselves, client engineers often hardcode data into views, leverage proxies to manipulate responses, or hack custom logic into the networking layer locally, resulting in wasted time and effort.Mocks get out of sync with GraphQL queries over time. Since most mocks are hand-written, they are not tightly coupled to the underlying queries and schema they are supposed to represent. If a team builds a new feature, then comes back a few months later to add new functionality backed by additional GraphQL fields, engineers must remember to manually update their mock data. As there is no forcing function to guarantee mocks stay in sync with queries, mock data tends to shift further away from the production reality as time passes — degrading the quality of tests.These challenges are not unique to Airbnb and are common across the industry. Although tooling like random value generators and local field resolvers can provide some assistance, they lack the domain knowledge and context needed to produce realistic, meaningful data for high-quality demos, quick product iteration, and reliable testing.GoalsWhen setting out to solve these challenges at Airbnb, we established three north-star goals:Eliminate the need to hand-write mock data. Mock data should be generated automatically to free up engineers from needing to hand-craft and maintain mock GraphQL data.Create highly realistic mock data. Mock data should match the user interface designs and look like real production data in order to support high-quality demos, which are highly valued at Airbnb for early feedback.Keep engineers in their local focus loops. Our solution should seamlessly integrate into engineers’ current development processes so they can generate mocks without context-switching to a website, separate repository, or unfamiliar tool.@generateMock: Schema + context + LLMs = magicTo generate mock data while keeping engineers in their local focus loops, we introduced a new client GraphQL directive called @generateMock, which engineers can use to automatically generate mock data for a given GraphQL operation, fragment, or field:Example of @generateMock being specified on a GraphQL query.This directive accepts a few optional arguments that engineers can use to customize the generated mock data, and the directive itself can be repeated with different input arguments to generate different mock variations:id: The identifier to use for the mock, as well as for naming generated helper functions. Useful when repeating the @generateMock directive to produce multiple mocks.hints: Additional context or instructions on how the mock should look. For example, a hint might be “Include travel entries for Barcelona, Paris, and Kyoto.” Under the hood, this information is fed to an LLM and heavily influences what the generated mock data looks like and how densely populated its fields are.designURL: The URL of a design mockup of the screen that will render the mock data. Specifying this argument helps the LLM produce mock data that matches the design by generating matching names, addresses, and other similar content.At Airbnb, engineers use a command line tool we call Niobe to generate code for their GraphQL queries and fragments. After modifying a .graphql file locally, engineers run this code generator, then use the generated TypeScript/Kotlin/Swift files to send GraphQL requests. To generate mock data using @generateMock, engineers simply need to run Niobe code generation after adding the directive — just as they would after making any other GraphQL change.During code generation, Niobe produces both a JSON file containing the actual mock data for each @generateMock directive, as well as a source file that provides functions for loading and consuming mock data from demo apps, snapshot tests, and unit tests. As shown in the Swift code below, the mockMixedStatusIndicators() function is generated on the InboxSyncQuery’s root Data type. It provides access to an instantiated type that’s populated with the generated mock data for mixed_status_indicators, allowing engineers to use the mock without having to load the JSON data manually:Using a generated mock in a Swift unit test.Engineers are free to modify the generated mock JSON data as well — as we’ll see below, Niobe will avoid overwriting their modifications on subsequent generation invocations.What does mock data look like?The context that we provide to the LLM is vital to generating data that is realistic enough to use in demos. To this end, Niobe collects the following information and includes it in the context passed to the LLM:The definitions of the query/fragment/fields being mocked (i.e., those marked with @generateMock and their dependencies).The subset of the GraphQL schema being queried, as well as any associated documentation that is present as inline comments. This information enables the LLM to infer the types that are used by the query being mocked. Importantly, this isn’t the whole schema, because including the full schema would likely overload the context window — Niobe traverses the schema and strips out types and fields that are not needed to resolve the query, along with any extra whitespace.The URL for the image representation of the design document specified within designURL, if any. Niobe integrates with an internal API to generate a snapshot image of the provided node in the design document. The API pushes this snapshot to a storage bucket and provides a URL that Niobe feeds to the LLM, along with specialized instructions on how to use it.The additional hints specified in @generateMock.The platform (e.g., “iOS”, “Android”, or “Web”) for which the mock data is being generated (for style specificity).A list of Airbnb-hosted image URLs that the LLM can choose from if needed, along with short textual descriptions of each. This prevents the LLM from hallucinating image URLs that don’t exist and ensures that the mock data contains valid URLs which can be properly loaded at runtime when prototyping or demoing.Illustration of the various pieces of context that are passed to the LLM during mock generation.All this information is consolidated into a prompt we fine-tuned against Gemini 2.5 Pro. We chose this model because of its 1-million token context window, plus the fact that in our internal tests this configuration performed significantly faster than comparable models while producing mock data of similar quality. Using this approach, we’re able to produce highly realistic JSON mocks which, when loaded into the application, yield very convincing results as shown below:Screenshot of a design mockup compared to a mock that was generated using @generateMock.The data in the screenshot on the right looks quite realistic, but if you look closely you may notice that the data is indeed mocked — all the photos are coming from the seed data set that we feed the LLM.How it worksWhen an engineer uses the Niobe CLI to generate code for their GraphQL files, Niobe automatically performs mock generation as the final step of this process, as shown in the flowchart below:If the @generateMock directive includes a designURL, Niobe validates the URL to ensure it includes a node-id, then uses an internal API to produce an image snapshot of that particular node. The API, in turn, pushes this snapshot to a storage bucket and provides Niobe with its URL.Next, the CLI aggregates all the context described in the section above — including the URL of the design snapshot — and crafts a prompt to send to the LLM. This prompt is then sent to the Gemini 2.5 Pro model, and results are streamed back to the client in order to show a progress indicator in the CLI.Once the mock JSON response has been received from the LLM, Niobe performs a validation step against this data by passing the GraphQL schema, client GraphQL document, and JSON data to the graphql NPM package’s graphqlSync function.If the validation produces errors (for example, if the LLM hallucinated an invalid enum value or failed to populate a required field), Niobe aggregates these errors and feeds them back into the LLM along with the initial mock data. This retry mechanism is used to essentially “self-heal” and fix invalid mock data.– This step is critical to reliably generating mock data. By placing the LLM within our existing GraphQL infrastructure, we’re able to enforce a set of guardrails through this validation step and provide strong guarantees that the mock data produced at the end of the pipeline is fully valid — something that wouldn’t be possible by using a tool outside our GraphQL infrastructure like ChatGPT.– Finally, once the mock data has been validated, Niobe writes it to a JSON file, alongside a companion source file which provides functions for loading the mock from application code.Flowchart of how mock generation works under the hood.@respondWithMock: Unblocking client developmentIn addition to generating realistic mock data with @generateMock, we also wanted to empower client engineers to iterate on features without waiting for the backend server implementation. A second directive, @respondWithMock, works alongside @generateMock to make this possible:Simple example of using the @respondWithMock directive.When this directive is present, Niobe alters the code that’s generated alongside the mock data to include extra details about this annotation. At runtime, the GraphQL client uses this to load the generated mock data, then seamlessly returns the mocked response instead of using data from the server. This effectively allows client engineers to unblock themselves from waiting on the server implementation, since they can easily use locally mocked data when querying unimplemented fields. The screenshot of the inbox screen earlier in this post is actually a real screenshot that was taken by generating with these two directives and running the Airbnb app in an iOS simulator — no manual mocking, proxying, or response modification needed!@respondWithMock can also be specified on individual fields. When used on fields within a query instead of on the query itself, the GraphQL client will actually request all fields from the server except those annotated with @respondWithMock, then patch in locally mocked data for the remaining fields — producing a hybrid of production and mock data, and making it possible for client engineers to develop against new (unimplemented) fields in existing queries. Engineers can even repeat this directive and use query input variables to decide if and when to return a specific generated mock at runtime, as shown below:Using @respondWithMock with conditionals and on individual fields.Schema evolution: Keeping mocks truthfulThe final challenge we addressed was the issue of keeping mocks in sync with queries as they evolve over time. Since Niobe manages mock data that is generated via the @generateMock directive, it can be smart about maintaining that mock data. As part of mock generation, Niobe embeds two extra keys in each generated JSON file:A hash of the client entity being mocked (i.e., the GraphQL query document).A hash of the input arguments to @generateMock.Niobe embeds version hashes in mock data in order to determine when a given mock needs to be updated.Each time code generation runs, Niobe determines whether existing mocks’ hashes differ from what their current hashes should be based on the GraphQL document. If they match, it skips mock generation for those types. On the other hand, if one of the hashes changed, Niobe intelligently updates that mock by including the existing mock in the context provided to the LLM, along with instructions on how to modify it.It’s important that Niobe doesn’t unnecessarily modify existing mock data for fields that are unchanged and still valid, since doing so could overwrite manual tweaks that were made to the JSON by engineers or break existing tests that rely on this data. To avoid this, we provide the LLM with a diff of what changed in the query, and tuned the prompt to focus on that diff and avoid making spurious changes to unrelated fields.Finally, each client codebase includes an automated check that ensures mock version hashes are up to date when code is submitted. This provides a guarantee that all generated mocks stay in sync with queries as they evolve over time. When engineers encounter these validation failures, they just re-run code generation locally — no manual updates required.Conclusion“@generateMock has significantly sped up my local development and made working with local data much more enjoyable.” — Senior Software EngineerBy integrating highly contextualized LLMs — informed by the GraphQL schema, product context, and UX designs — directly into existing GraphQL tooling, we’ve unlocked the ability to generate valid and realistic mock data while eliminating the need for engineers to manually hand-write and maintain mocks. The directive-driven approach of @generateMock and @respondWithMock allows engineers to build clients before the server implementation is complete while keeping them in their focus loops and providing a guarantee that mock data stays in sync as queries evolve.In just the past few months, Airbnb engineers have generated and merged over 700 mocks across iOS, Android, and Web using @generateMock, and we plan to roll out internal support for backend services soon. These tools have fundamentally changed how engineers mock GraphQL data for tests and prototypes at Airbnb, allowing them to focus on building product features rather than crafting and maintaining mock data.AcknowledgmentsSpecial thanks to Raymond Wang and Virgil King for their contributions bringing @generateMock support to Web and Android clients, as well as to many other engineers and teams at Airbnb who participated in design reviews, built supporting infrastructure, and provided usage feedback.Does this type of work interest you? Check out our open roles here.GraphQL Data Mocking at Scale with LLMs and @generateMock was originally published in The Airbnb Tech Blog on Medium, where people are continuing the conversation by highlighting and responding to this story."
    },
    {
      "title": "“Systems thinking helps me put the big picture front and center”",
      "published": "2025-10-30 14:34:25",
      "link": "https://towardsdatascience.com/systems-thinking-helps-me-put-the-big-picture-front-and-center/",
      "summary": "Shuai Guo on deep research agents, analytical AI vs LLM-based agents, and systems thinking\nThe post “Systems thinking helps me put the big picture front and center” appeared first on Towards Data Science."
    },
    {
      "title": "Expanding Stargate to Michigan",
      "published": "2025-10-30 13:30:00",
      "link": "https://openai.com/index/expanding-stargate-to-michigan",
      "summary": "OpenAI is expanding Stargate to Michigan with a new one-gigawatt campus that strengthens America’s AI infrastructure. The project will create jobs, drive investment, and support economic growth across the Midwest."
    },
    {
      "title": "AI Model Growth Outpaces Hardware Improvements",
      "published": "2025-10-30 13:00:00",
      "link": "https://spectrum.ieee.org/mlperf-trends",
      "summary": "Since 2018, the consortium MLCommons has been running a sort of Olympics for AI training. The competition, called MLPerf, consists of a set of tasks for training specific AI models, on predefined datasets, to a certain accuracy. Essentially, these tasks, called benchmarks, test how well a hardware and low-level software configuration is set up to train a particular AI model. Twice a year, companies put together their submissions—usually, clusters of CPUs and GPUs and software optimized for them—and compete to see whose submission can train the models fastest. There is no question that since MLPerf’s inception, the cutting-edge hardware for AI training has improved dramatically. Over the years, Nvidia has released four new generations of GPUs that have since become the industry standard (the latest, Nvidia’s Blackwell GPU, is not yet standard but growing in popularity). The companies competing in MLPerf have also been using larger clusters of GPUs to tackle the training tasks.However, the MLPerf benchmarks have also gotten tougher. And this increased rigor is by design—the benchmarks are trying to keep pace with the industry, says David Kanter, head of MLPerf. “The benchmarks are meant to be representative,” he says. Intriguingly, the data show that the large language models and their precursors have been increasing in size faster than the hardware has kept up. So each time a new benchmark is introduced, the fastest training time gets longer. Then, hardware improvements gradually bring the execution time down, only to get thwarted again by the next benchmark. Then the cycle repeats itself."
    },
    {
      "title": "Introducing Aardvark: OpenAI’s agentic security researcher",
      "published": "2025-10-30 11:00:00",
      "link": "https://openai.com/index/introducing-aardvark",
      "summary": "OpenAI introduces Aardvark, an AI-powered security researcher that autonomously finds, validates, and helps fix software vulnerabilities at scale. The system is in private beta—sign up to join early testing."
    },
    {
      "title": "Toward provably private insights into AI use",
      "published": "2025-10-30 10:56:00",
      "link": "https://research.google/blog/toward-provably-private-insights-into-ai-use/",
      "summary": "Generative AI"
    },
    {
      "title": "From Bottleneck to Breakthrough: AI in Chip Verification",
      "published": "2025-10-30 10:00:05",
      "link": "https://spectrum.ieee.org/calibre-vision-ai-chip-design",
      "summary": "This is a sponsored article brought to you by Siemens.In the world of electronics, integrated circuits (IC) chips are the unseen powerhouse behind progress. Every leap—whether it’s smarter phones, more capable cars, or breakthroughs in healthcare and science—relies on chips that are more complex, faster, and packed with more features than ever before. But creating these chips is not just a question of sheer engineering talent or ambition. The design process itself has reached staggering levels of complexity, and with it, the challenge to keep productivity and quality moving forward.As we push against the boundaries of physics, chipmakers face more than just technical hurdles. The workforce challenges, tight timelines, and the requirements for building reliable chips are stricter than ever. Enormous effort goes into making sure chip layouts follow detailed constraints—such as maintaining minimum feature sizes for transistors and wires, keeping proper spacing between different layers like metal, polysilicon, and active areas, and ensuring vias overlap correctly to create solid electrical connections. These design rules multiply with every new technology generation. For every innovation, there’s pressure to deliver more with less. So, the question becomes: How do we help designers meet these demands, and how can technology help us handle the complexity without compromising on quality?Shifting the paradigm: the rise of AI in electronic design automationA major wave of change is moving through the entire field of electronic design automation (EDA), the specialized area of software and tools that chipmakers use to design, analyze, and verify the complex integrated circuits inside today’s chips. Artificial intelligence is already touching many parts of the chip design flow—helping with placement and routing, predicting yield outcomes, tuning analog circuits, automating simulation, and even guiding early architecture planning. Rather than simply speeding up old steps, AI is opening doors to new ways of thinking and working.Machine learning models can help predict defect hotspots or prioritize risky areas long before sending a chip to be manufactured.Instead of brute-force computation or countless lines of custom code, AI uses advanced algorithms to spot patterns, organize massive datasets, and highlight issues that might otherwise take weeks of manual work to uncover. For example, generative AI can help designers ask questions and get answers in natural language, streamlining routine tasks. Machine learning models can help predict defect hotspots or prioritize risky areas long before sending a chip to be manufactured.This growing partnership between human expertise and machine intelligence is paving the way for what some call a “shift left” or concurrent build revolution—finding and fixing problems much earlier in the design process, before they grow into expensive setbacks. For chipmakers, this means higher quality and faster time to market. For designers, it means a chance to focus on innovation rather than chasing bugs.  Figure 1. Shift-left and concurrent build of IC chips performs multiple tasks simultaneously that use to be done sequentially.SiemensThe physical verification bottleneck: why design rule checking is harder than everAs chips grow more complex, the part of the design called physical verification becomes a critical bottleneck. Physical verification checks whether a chip layout meets the manufacturer’s strict rules and faithfully matches the original functional schematic. Its main goal is to ensure the design can be reliably manufactured into a working chip, free of physical defects that might cause failures later on.Design rule checking (DRC) is the backbone of physical verification. DRC software scans every corner of a chip’s layout for violations—features that might cause defects, reduce yield, or simply make the design un-manufacturable. But today’s chips aren’t just bigger; they’re more intricate, woven from many layers of logic, memory, and analog components, sometimes stacked in three dimensions. The rules aren’t simple either. They may depend on the geometry, the context, the manufacturing process and even the interactions between distant layout features.  Priyank Jain leads product management for Calibre Interfaces at Siemens EDA.SiemensTraditionally, DRC is performed late in the flow, when all components are assembled into the final chip layout. At this stage, it’s common to uncover millions of violations—and fixing these late-stage issues requires extensive effort, leading to costly delays.To minimize this burden, there’s a growing focus on shifting DRC earlier in the flow—a strategy called “shift-left.” Instead of waiting until the entire design is complete, engineers try to identify and address DRC errors much sooner at block and cell levels. This concurrent design and verification approach allows the bulk of errors to be caught when fixes are faster and less disruptive.However, running DRC earlier in the flow on a full chip when the blocks are not DRC clean produces results datasets of breathtaking scale—often tens of millions to billions of “errors,” warnings, or flags because the unfinished chip design is “dirty” compared to a chip that’s been through the full design process. Navigating these “dirty” results is a challenge all on its own. Designers must prioritize which issues to tackle, identify patterns that point to systematic problems, and decide what truly matters. In many cases, this work is slow and “manual,” depending on the ability of engineers to sort through data, filter what matters, and share findings across teams.To cope, design teams have crafted ways to limit the flood of information. They might cap the number of errors per rule, or use informal shortcuts—passing databases or screenshots by email to team members, sharing filters in chat messages, and relying on experts to know where to look. Yet this approach is not sustainable. It risks missing major, chip-wide issues that can cascade through the final product. It slows down response and makes collaboration labor-intensive.With ongoing workforce challenges and the surging complexity of modern chips, the need for smarter, more automated DRC analysis becomes urgent. So what could a better solution look like—and how can AI help bridge the gap?The rise of AI-powered DRC analysisRecent breakthroughs in AI have changed the game for DRC analysis in ways that were unthinkable even a few years ago. Rather than scanning line by line or check by check, AI-powered systems can process billions of errors, cluster them into meaningful groups, and help designers find the root causes much faster. These tools use techniques from computer vision, advanced machine learning, and big data analytics to turn what once seemed like an impossible pile of information into a roadmap for action.AI’s ability to organize chaotic datasets—finding systematic problems hidden across multiple rules or regions—helps catch risks that basic filtering might miss. By grouping related errors and highlighting hot spots, designers can see the big picture and focus their time where it counts. AI-based clustering algorithms reliably transform weeks of manual investigation into minutes of guided analysis.AI-powered systems can process billions of errors, cluster them into meaningful groups, and help designers find the root causes much faster.Another benefit: collaboration. By treating results as shared, living datasets—rather than static tables—modern tools let teams assign owners, annotate findings and pass exact analysis views between block and partition engineers, even across organizational boundaries. Dynamic bookmarks and shared UI states cut down on confusion and rework. Instead of “back and forth,” teams move forward together.Many of these innovations tease at what’s possible when AI is built into the heart of the verification flow. Not only do they help designers analyze the results; they help everyone reason about the data, summarize findings and make better design decisions all the way to tape out.A real-world breakthrough in DRC analysis and collaboration: Siemens’ Calibre Vision AIOne of the most striking examples of AI-powered DRC analysis comes from Siemens, whose Calibre Vision AI platform is setting new standards for how full-chip verification happens. Building on years of experience in physical verification, Siemens realized that breaking bottlenecks required not only smarter algorithms but rethinking how teams work together and how data moves across the flow.Vision AI is designed for speed and scalability. It uses a compact error database and a multi-threaded engine to load millions—or even billions—of errors in minutes, visualizing them so engineers see clusters and hot spots across the entire die. Instead of a wall of error codes or isolated rule violations, the tool presents a heat map of the layout, highlighting areas with the highest concentration of issues. By enabling or disabling layers (layout, markers, heat map) and adjusting layer opacity, users get a clear, customizable view of what’s happening—and where to look next.Using advanced machine learning algorithms, Vision AI analyzes every error to find groups with common failure causes. But the real magic is in AI-guided clustering. Using advanced machine learning algorithms, Vision AI analyzes every error to find groups with common failure causes. This means designers can attack the root cause once, fixing problems for hundreds of checks at a time instead of tediously resolving them one by one. In cases where legacy tools would force teams to slog through, for example, 3,400 checks with 600 million errors, Vision AI’s clustering can reduce that effort to investigating just 381 groups—turning mountains into molehills and speeding debug time by at least 2x.  Figure 2. The Calibre Vision AI software automates and simplifies the chip-level DRC verification process.SiemensVision AI is also highly collaborative. Dynamic bookmarks capture the exact state of analysis, from layer filters to zoomed layout areas, along with annotations and owner assignments. Sharing a bookmark sends a living analysis—not just a static snapshot—to coworkers, so everyone is working from the same view. Teams can export results databases, distribute actionable groups to block owners, and seamlessly import findings into other Siemens EDA tools for further debug.Empowering every designer: reducing the expertise gapA frequent pain point in chip verification is the need for deep expertise—knowing which errors matter, which patterns mean trouble, and how to interpret complex results. Calibre Vision AI helps level the playing field. Its AI-based algorithms consistently create the same clusters and debug paths that senior experts would identify, but does so in minutes. New users can quickly find systematic issues and perform like seasoned engineers, helping chip companies address workforce shortages and staff turnover.Beyond clusters and bookmarks, Vision AI lets designers build custom signals by leveraging their own data. The platform secures customer models and data for exclusive use, making sure sensitive information stays within the company. And by integrating with Siemens’ EDA AI ecosystem, Calibre Vision AI supports generative AI chatbots and reasoning assistants. Designers can ask direct questions—about syntax, about a signal, about the flow—and get prompt—accurate answers, streamlining training and adoption.Real results: speeding analysis and sharing insightCustomer feedback from leading IC companies shows the real-world value of AI for full-chip DRC analysis and debug. One company reported that Vision AI reduced their debug effort by at least half—a gain that makes the difference between tapeout and delay. Another noted the platform’s signals algorithm automatically creates the same check groups that experienced users would manually identify, saving not just time but energy.Quantitative gains are dramatic. For example, Calibre Vision AI can load and visualize error files significantly faster than traditional debug flows. Figure 3 shows the difference in four different test cases: a results file that took 350 minutes with the traditional flow, took Calibre Vision AI only 31 minutes. In another test case (not shown), it took just five minutes to analyze and cluster 3.2 billion errors from more than 380 rule checks into 17 meaningful groups. Instead of getting lost in gigabytes of error data, designers now spend time solving real problems.  Figure 3. Charting the results load time between the traditional DRC debug flow and the Calibre Vision AI flow.SiemensLooking ahead: the future of AI in chip designToday’s chips demand more than incremental improvements in EDA software. As the need for speed, quality and collaboration continues to grow, the story of physical verification will be shaped by smarter, more adaptive technologies. With AI-powered DRC analysis, we see a clear path: a faster and more productive way to find systematic issues, intelligent debug, stronger collaboration and the chance for every designer to make an expert impact.By combining the creativity of engineers with the speed and insight of AI, platforms like Calibre Vision AI are driving a new productivity curve in full-chip analysis. With these tools, teams don’t just keep up with complexity—they turn it into a competitive advantage.At Siemens, the future of chip verification is already taking shape—where intelligence works hand in hand with intuition, and new ideas find their way to silicon faster than ever before. As the industry continues to push boundaries and unlock the next generation of devices, AI will help chip design reach new heights.For more on Calibre Vision AI and how Siemens is shaping the future of chip design, visit eda.sw.siemens.com and search for Calibre Vision AI."
    }
  ]
}