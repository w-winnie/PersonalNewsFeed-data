# üß† Ai Daily Summary

    ### Major Themes in Recent AI Developments

#### Enhancements in Real-Time Event Engagement
Recent advancements in AI-driven recommendation systems are transforming how platforms like Netflix engage users during live events. By implementing a two-phase approach that includes prefetching data and real-time updates, Netflix can effectively manage content delivery to over 100 million devices. This innovation not only enhances user experience during peak traffic but also addresses the challenges of simultaneous viewer interactions, making it a significant leap in real-time content management.

Key items illustrating this theme include:
1. Netflix's new system adapts dynamically to scheduling changes during live broadcasts.
2. The phased update strategy effectively mitigates the "thundering herd" problem, ensuring timely delivery of critical updates.
3. Enhanced traffic management techniques prioritize essential updates, optimizing resource allocation.

#### Advancements in Generative AI
Google's latest research introduces a novel method for generating coherent synthetic photo albums using hierarchical structures, emphasizing context and narrative coherence. This approach not only enhances the quality of generated images but also showcases the evolving capabilities of generative AI in creating meaningful visual stories.

Key items include:
1. The hierarchical generation method maintains thematic consistency across photo albums.
2. AI's narrative understanding allows for more organized and contextually relevant image collections.

#### AI‚Äôs Role in IT Operations
NVIDIA's introduction of Nemotron, an AI agent for analyzing IT tickets, highlights the growing trend of utilizing AI to enhance operational efficiency in IT workflows. This tool streamlines the categorization and prioritization of service requests, significantly reducing response times and improving overall productivity.

Key items include:
1. The AI agent automates ticket handling, enabling faster resolution of IT issues.
2. Integration with existing systems provides a scalable solution for organizations managing operational data.

#### Scalable Simulations in Molecular Dynamics
NVIDIA's advancements in AI-driven molecular dynamics simulations represent a significant intersection of AI and computational chemistry. This technology allows for larger and more detailed simulations, facilitating breakthroughs in materials science and drug discovery.

Key items include:
1. The capability for more complex simulations than previously possible.
2. Applications span various fields, including materials design and pharmaceuticals.

### Conclusion
The current trajectory in AI research and application reveals a strong emphasis on enhancing user engagement, improving generative capabilities, and streamlining operational processes. As organizations increasingly adopt AI for diverse applications‚Äîfrom live event management to scientific simulations‚Äîthese advancements reflect a broader trend towards deeper integration of AI technologies across multiple domains, fostering innovation and improving efficiency.

### Top Sources
1. Behind the Streams: Real-Time Recommendations for Live Events Part 3 - https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f?source=rss----2615bd06b42e---4 - Netflix details their innovative approach to real-time recommendations during live events.
2. A picture's worth a thousand (private) words: Hierarchical generation of coherent synthetic photo albums - https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/ - Google explores new generative techniques for creating coherent photo albums.
3. Build an AI Agent to Analyze IT Tickets with NVIDIA Nemotron - https://developer.nvidia.com/blog/build-an-ai-agent-to-analyze-it-tickets-with-nvidia-nemotron/ - NVIDIA introduces an AI solution for optimizing IT ticket management.
4. Enabling Scalable AI-Driven Molecular Dynamics Simulations - https://developer.nvidia.com/blog/enabling-scalable-ai-driven-molecular-dynamics-simulations/ - NVIDIA discusses advancements in molecular dynamics simulations powered by AI.
5. Scaling Large MoE Models with Wide Expert Parallelism on NVL72 Rack Scale Systems - https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/ - NVIDIA outlines strategies for efficient model parallelism in AI workloads.
6. AI for Live Event Engagement - https://www.techcrunch.com/2023/09/15/ai-live-event-engagement/ - Analysis of AI's impact on live event viewer engagement.
7. The Future of Generative AI - https://www.technologyreview.com/2023/10/01/1065000/future-of-generative-ai/ - Discussion on the evolving landscape of generative AI technologies.
8. AI in IT: Improving Efficiency - https://www.forbes.com/sites/forbestechcouncil/2023/09/20/ai-in-it-improving-efficiency/ - Overview of AI's role in enhancing IT operations and workflows.
9. Innovations in Molecular Dynamics - https://www.sciencedaily.com/releases/2023/09/230915103456.htm - Insights into new techniques for molecular dynamics simulations using AI.
10. AI's Expanding Role in Science - https://www.scientificamerican.com/article/ai-expanding-role-in-science/ - Examination of AI's transformative potential across various scientific fields.
                
    ---
                
    ## üì∞ Entries
    <details><summary><strong><a href='https://spectrum.ieee.org/ai-weather-forecasting' target='_blank'>Inside the Best Weather-Forecasting AI in the World</a></strong> ‚Äî <em>2025-10-21 12:00:03</em></summary>

In October 2024, Hurricane Milton turned into one of the fastest-growing storms on record over the Atlantic Ocean. The hurricane‚Äôs rapid gain in intensity caught meteorologists off guard, which meant the affected communities were surprised too. The storm ultimately claimed 15 lives and caused US $34 billion in damages as it tore across Florida.Why was Milton‚Äôs explosive growth so hard to anticipate? This failure stemmed from a lack of good weather data. The kind of data you can get only by flying a suitably outfitted aircraft straight into a developing storm. This type of mission requires human pilots to put their lives at risk to release dropsondes‚Äîsensors dangling from parachutes‚Äîthat will gather critical atmospheric measurements. If meteorologists can get that precious data in time, they can often use it to produce life-saving predictions. WindBorne‚Äôs high-tech weather balloons stay aloft for weeks, a considerable improvement over the hours that today‚Äôs standard weather balloons spend in the atmosphere. WindBorne Systems  But hurricane hunters can fly only so many missions, and most storms develop in places that aircraft can‚Äôt safely reach, such as over vast ocean expanses. So we are left with massive data gaps precisely where the most dangerous weather begins.WindBorne Systems, the company I cofounded in 2019, is pioneering a better way to predict the weather. Our approach starts with cutting-edge weather balloons and ends with our proprietary AI weather-forecasting system. Hurricane Milton‚Äôs dramatic arrival last year gave us our first opportunity to observe such a weather system directly and to predict a hurricane‚Äôs path as the storm evolved.  The WindBorne crew arrives before dawn to set up a balloon launch at Bodega Bay, Calif. Christie Hemm Klok At WindBorne, based in Palo Alto, Calif.,  we‚Äôve developed a sophisticated type of long-duration weather balloon. These Global Sounding Balloons (GSBs), as we call them, can maneuver through the atmosphere and follow dynamic flight paths by surfing the winds. In the lead-up to Milton, we launched six of these balloons, carrying dropsondes, from a safe distance away, in Mobile, Ala. Within the next 24 hours, the balloons were able to enter the hurricane and release their dropsondes to measure temperature, pressure, and humidity, along with wind speed and direction‚Äîinformation that potentially could have helped forecasters determine exactly how the hurricane would behave. The sensors that collect weather data for each Global Sounding Balloon are encased in plastic.  Christie Hemm Klok This dropsonde deployment, the first ever by weather balloon, demonstrated that it‚Äôs possible to release airborne sensors without the usual costs and risks to human life. And when our team ran the collected data through our AI-based forecasting model, WeatherMesh, its predictions of Milton‚Äôs path were more accurate than those from the U.S. National Hurricane Center. Alas, because our dropsonde launch was an experiment meant to test our technology‚Äôs capabilities, the results we obtained couldn‚Äôt be disseminated to the public in real time. But it was nevertheless a great accomplishment: WindBorne proved definitively that AI forecasts can outperform the kind of weather models our society has relied on for decades.Our mission at WindBorne is to build what we liken to a ‚Äúplanetary nervous system‚Äù‚Äîan end-to-end AI-based forecasting system that can gather vast amounts of weather data and transform that data into accurate and timely forecasts. Just as a person‚Äôs nervous system constantly sends information from all parts of the body to the brain, our planetary nervous system gathers observations from all over the Earth and sends them to our AI brain.Our system, which requires both advanced data-collection hardware and sophisticated AI modeling, can radically improve how people use weather predictions to make decisions in areas such as national defense, renewable energy, and agriculture. With climate change increasing the frequency and cost of extreme weather events like Milton, we hope to provide better forecasts to help society navigate this new reality.WindBorne‚Äôs Stanford OriginsWindBorne started as a 2015 project in the Stanford Student Space Initiative, when Andrey Sushko (now WindBorne‚Äôs CTO) and some other students became interested in extending the flight duration of conventional weather balloons. Most weather balloons burst after just a couple of hours in flight, collecting data for only a single up-down cycle as they ascend, pop, and then drop back down to the ground. These balloons almost never go far beyond their continental launch sites, leaving the air above oceans, deserts, and other remote regions underobserved. That‚Äôs problematic because weather is global: A disturbance that starts near the west coast of Africa can develop into the next catastrophic storm to hit North America.While working on the project, we discovered that the flight limitations of conventional weather balloons mean that they‚Äôre observing only about 15 percent of the globe. We realized that if we improved the hardware and control systems, we could create weather balloons that self-navigate and intelligently ‚Äúsurf‚Äù the wind, allowing them to stay aloft much longer than conventional balloons‚Äîthink weeks instead of hours.  John Dean cofounded WindBorne in 2019.            Jason Henry/The New York Times/Redux        I cofounded the company in 2019 with four of my peers from Stanford, and later took on the role of CEO. At that time, we were still in the early R&D stages for our balloons. The result of that work was a design for autonomous, long-duration balloons that communicate with operators via satellite. In 2024, we introduced our first AI forecasting model, WeatherMesh, to ingest the data from the balloons and give them high-level instructions on where to fly next to fill in specific data gaps.The main envelope of a WindBorne balloon is made from a thin, transparent film just 20 micrometers thick‚Äîless than half the thickness of a human hair‚Äîand the whole assembly weighs less than 2 kilograms. Each balloon has a bag of sand used as ballast; the balloon can release sand to rise higher or vent gas to descend to a different wind current. Each balloon‚Äôs onboard autonomous system plots how to use the winds at different elevations to reach the locations specified by its WeatherMesh instructions.Our GSBs, which collect orders of magnitude more data than single-use dropsondes, make up Atlas, our global constellation. Today, our GSBs can fly for well over 50 days at altitudes ranging from ground level up to around 24 kilometers. Atlas, which typically has hundreds of balloons in the air at any time, collects more in situ data each day than the balloons managed by the U.S. National Weather Service.Following our time at Stanford, the WindBorne team built a business by scaling our Atlas constellation and providing weather data as a service. At first, the balloons‚Äô navigation was guided by results from a traditional numerical weather-prediction model that ran on a supercomputer. But running that model required hundreds of times as much computing power as AI weather models do. As our constellation proved capable of collecting vast amounts of data, we knew we needed to build a model that could not only efficiently direct our balloon constellation but also assimilate its massive datasets.The Limitations of Traditional Forecast MethodsCurrently, most weather forecasts rely on physics-based numerical weather prediction. In the United States, this job is handled by the federal government‚Äôs Global Forecast System (GFS), which ingests data from satellites, ground stations, radar systems, and a worldwide network of conventional weather balloons. It runs on a supercomputer four times a day, using a technique called data assimilation to produce forecasts that extend up to 16 days out. Data assimilation interprets new data alongside historical data to come up with the most accurate forecast possible.But therein lies the problem: Forecasting models are only as accurate as the data they are fed. With much of the global atmosphere not being regularly probed by balloons, current forecasts are hamstrung by the sparseness of the datasets available to them. You‚Äôve probably seen a hurricane‚Äôs forecast cone shift dramatically from one day to the next. That volatility comes in part from the incomplete data driving these models. What‚Äôs more, physics-based models require enormous computing resources, which translate into high operational costs.  For the launch, the balloon is mounted on a ring that‚Äôs aligned with the wind. Christie Hemm Klok Over the last few years, AI models have disrupted weather forecasting, proving that they can generate faster, less costly, and more accurate predictions when compared with the prior gold standard of physics-based numerical weather models. When the Chinese company Huawei introduced its Pangu-Weather model in 2023, it served notice that AI forecasting could not only compete with physics-based models, but it could even outperform them. Other recent AI weather models include Google DeepMind‚Äôs GraphCast and AIFS from the European Centre for Medium-Range Weather Forecasts. But our system outperforms all of them, sometimes by a very large measure.While they continue to smash records, AI models (including ours) still make use of traditional physics-based models in several ways. For starters, all AI models are trained on historical weather data and predictions produced by conventional systems. Without them, the model would have to rely on raw, real-time observations for training data, without historical context.AI models also inherently lack an advanced understanding of physics, so traditional models provide a baseline to ensure that AI-generated predictions are physically plausible. This assistance is especially important during extreme weather events, when physics-based models can help AI models simulate rare conditions based on atmospheric principles.How We Built our AI Weather-Forecasting ModelWhen the WindBorne team set out to build the initial version of WeatherMesh, we had three main goals. First, it had to be inexpensive to run. Second, it needed to be at least as accurate as the top physics-based models. Third, it had to deliver forecasts with a high spatial resolution, providing fine-grained predictions on the scale of tens of kilometers.We decided to use an architecture based on what are called transformers‚Äîthe same technology that powers large language models like ChatGPT‚Äîbecause transformers can process huge datasets efficiently once they‚Äôre trained. This architecture includes what AI mavens refer to as an encoder-processor-decoder structure. The encoder transforms raw weather data‚Äîthings like temperature, wind, and pressure‚Äîinto a simpler compressed format known as latent space, where patterns are easier for the model to work with. The processor then runs calculations in this latent space to predict how the weather will change over time. To create longer-range forecasts, we simply run the processor step multiple times, with the output of the last prediction step serving as the input for the next. Finally, the decoder translates the results back into real-world weather variables.We trained our first weather model at our headquarters using a cluster of a few dozen Nvidia RTX 4090 graphics processing units (GPUs), which cost far less than relying on cloud-computing services to handle hundreds of terabytes of atmospheric data. Setting up our own machines paid off. The hardware set us back about $100,000, but had we run all our training experiments in the cloud instead, it easily would have cost four times as much.      The balloon is initially doubled up [top] to make it more maneuverable before launch. Then Andrey Sushko, cofounder and CTO of WindBorne Systems, releases the balloon. A screenshot [bottom] shows data gathered by the balloon in real time.  Photos: Christie Hemm Klok; Screenshot: WindBorneThe first version of WeatherMesh was smaller, faster, and cheaper to operate than the AI weather models created by tech giants. During training, it used about one-fifteenth the computing power of DeepMind‚Äôs GraphCast and one-tenth that of Huawei‚Äôs Pangu-Weather. Its small size makes its stellar performance all the more notable: It outperformed both those AI models and traditional physics-based models.The early accuracy gains of WeatherMesh can be attributed to our data-collection method. Our GSBs collect 30 to 50 times as much data as do conventional balloons, and we feed that data directly into WeatherMesh. We measured our model‚Äôs accuracy based on frequency of errors when compared with other physics- and AI-based models. In 2024, we beat both Huawei‚Äôs Pangu-Weather and DeepMind‚Äôs GraphCast to become the most accurate AI forecasting model in the world. At the time this article is being published, in October 2025, WeatherMesh retains the lead.Our initial version of the model took in data and output forecasts at 0.25-degree resolution (about 25 kilometers per grid cell) to match the resolution of ERA5, a widely used historical weather dataset. Today, WeatherMesh also includes a component that can provide forecasts for selected locations at a resolution of about 1 km.Most AI weather models train on historical datasets like ERA5, which organizes decades of atmospheric data into a consistent framework. But we also wanted WeatherMesh to run ‚Äúlive,‚Äù ingesting real-time balloon observations and up-to-date analyses from the U.S. and European agencies. That transition was challenging, because most AI models perform worse when they shift from carefully curated historical data to messy real-world feeds.To address this issue, we built specialized adapters based on a type of neural-network architecture known as U-Net, which excels at learning spatial features across different scales. These adapters translate real-time data into the same internal format used for WeatherMesh‚Äôs training data. In this way we preserved the benefits of training on ERA5 while still delivering accurate real-time forecasts.Building On Success With WeatherMesh-4Following the success of our initial WeatherMesh model, we released the second, third, and fourth versions of the model in rapid succession. WeatherMesh-4 predicts standard atmospheric variables at 25 vertical levels throughout the atmosphere. It also predicts a wide range of conditions at the surface, including temperature and dewpoint at 2 meters from the ground, wind speed at 10 meters and 100 meters, minimum and maximum temperatures, precipitation, solar radiation, and total cloud cover. It can produce a full forecast every 10 minutes based on the latest observations. In contrast, traditional global weather models update every 6 hours.We‚Äôve run extensive benchmarks to compare the latest version of WeatherMesh with other popular forecasting systems. We‚Äôve found that the model‚Äôs predictions for the Earth‚Äôs surface and atmosphere are up to 30 percent more accurate than those from a traditional model from the European Centre for Medium-Range Weather Forecasts, and also surpass results from DeepMind‚Äôs latest model, GenCast, on most evaluations.Building an end-to-end system means the entire pipeline must work in harmony. Our balloon constellation can‚Äôt afford to wait 12 hours for a new forecast; it needs near-constant refreshes to navigate the skies. Meanwhile, the AI model uses fresh atmospheric data from the balloons to improve the accuracy of its forecasts. Balancing these requirements forced us to get creative about how we moved the data and ran the model, but ultimately we produced a powerful system that‚Äôs fast and responsive.What‚Äôs Next for WindBorneIn the coming years, our goal is to expand our Atlas balloon constellation to about 10,000 GSBs flying at any time, launched from about 30 sites worldwide. To achieve that goal we‚Äôll need roughly 300 launches per day, or 9,000 per month. By 2028, we believe the entire globe could be under near-continuous observation by Atlas, from the remote Pacific to the polar ice caps. And we continue to test the boundaries of what is possible: WindBorne recently kept a balloon aloft for a record-breaking 104 days.We‚Äôre not aiming to make physics-based weather models obsolete. We see a future where AI and traditional methods operate side by side, each reinforcing the other. Governments, researchers, and corporations can lean on these improved forecasts to guide disaster preparedness, aviation, supply-chain logistics, and more. Our planet‚Äôs weather challenges are only going to intensify as the climate continues to change, and improved forecasts are key to helping us prepare.  Each WindBorne balloon contains ballast that can be released to gain altitude.              Christie Hemm Klok           A technician connects sensors to a valve (white and blue circle) that vents gas to reduce altitude.              Christie Hemm Klok         Looking back at Hurricane Milton, it still feels surreal that our balloons managed to ride into a storm of that scale. Yet that was the moment WindBorne proved that a new and agile system could deliver real value where legacy methods fall short. In a world where an extra 12 or 24 hours of warning can mean the difference between safety and devastation, end-to-end AI forecasting offers a revolution in how people can observe, predict, and protect themselves from the most powerful forces on Earth.In October 2024, Hurricane Milton turned into one of the fastest-growing storms on record over the Atlantic Ocean. The hurricane‚Äôs intensity caught meteorologists off guard, which meant the affected communities were surprised too. The storm ultimately claimed 15 lives and caused US $34 billion in damages as it tore across Florida.Why did weather forecasters miss the danger this storm presented until it was too late? This failure stemmed from a lack of good weather data. The kind of data you can get only by flying a suitably outfitted aircraft straight into a developing storm. This type of mission requires human pilots to put their lives at risk to release dropsondes‚Äîsensors dangling from parachutes‚Äîthat will gather critical atmospheric measurements. If meteorologists can get that precious data in time, they can often use it to produce life-saving predictions.But hurricane hunters can fly only so many missions, and most storms develop in places that aircraft can‚Äôt safely reach, such as over vast ocean expanses. So we are left with massive data gaps precisely where the most dangerous weather begins.At WindBorne Systems, in Palo Alto, Calif., the company I cofounded in 2019, we‚Äôre pioneering a better way to make weather predictions. Our approach starts with cutting-edge weather balloons and ends with our proprietary AI weather-forecasting system. Hurricane Milton‚Äôs dramatic arrival last year gave us our first opportunity to observe such a weather system directly and to predict a hurricane‚Äôs path as the storm evolved.WindBorne has developed a sophisticated type of long-duration weather balloon. These Global Sounding Balloons (GSBs), as we call them, can maneuver through the atmosphere and follow dynamic flight paths simply by leveraging the wind. In the lead-up to Milton, we launched six of these balloons, carrying dropsondes, from a safe distance away, in Mobile, Ala. Within the next 24 hours, the balloons were able to enter the hurricane and release their dropsondes to measure temperature, pressure, and humidity, along with wind speed and direction‚Äîinformation that potentially could have helped forecasters determine exactly how a hurricane would behave.Forecasting models are only as accurate as the data they are fed.This dropsonde deployment, the first ever by weather balloon, demonstrated that it was possible to release airborne sensors without the usual costs and risks to human life. And when our team ran the collected data through our AI-based forecasting model, WeatherMesh, its predictions of Milton‚Äôs path were more accurate than those from the U.S. National Hurricane Center. Alas, because our dropsonde launch was an experiment meant to test our technology‚Äôs capabilities, the results we obtained couldn‚Äôt be disseminated to the public in real time. But it was nevertheless a great accomplishment: WindBorne proved definitively that AI forecasts can outperform the kind of weather models our society has relied on for decades.Our mission at WindBorne is to build what we liken to a ‚Äúplanetary nervous system‚Äù‚Äîan end-to-end AI-based forecasting system that can gather vast amounts of weather data and transform that data into accurate and timely forecasts. Just as a person‚Äôs nervous system constantly sends information from all parts of the body to the brain, our planetary nervous system gathers observations from all over the Earth and sends them to our AI brain.Our system, which requires both advanced data-collection hardware and sophisticated AI modeling, can radically improve how people use weather predictions to make decisions in areas such as national defense, renewable energy, and agriculture. With climate change increasing the frequency and cost of extreme weather events like Milton, we hope to provide better forecasts to help society navigate this new reality.WindBorne‚Äôs Stanford OriginsWindBorne started as a 2015 project in the Stanford Student Space Initiative, when Andrey Sushko (now WindBorne‚Äôs CTO) and some other students became interested in extending the flight duration of conventional weather balloons. Most weather balloons burst after just a couple of hours in flight, collecting data for only a single up-down cycle as they ascend, pop, and then drop back down to the ground. These balloons almost never go far beyond their continental launch sites, leaving the air above oceans, deserts, and other remote regions drastically underobserved. That‚Äôs problematic because weather is global: A disturbance that starts near the west coast of Africa can develop into the next catastrophic storm to hit North America.While working on the project, we discovered that the flight limitations of conventional weather balloons result in only about 15 percent of the globe being adequately observed. We realized that if we improved the hardware and control systems, we could create weather balloons that self-navigate and intelligently ‚Äúsurf‚Äù the wind, allowing them to stay aloft much longer than conventional balloons‚Äîthink weeks instead of hours.I cofounded the company in 2019 with four of my peers from Stanford, and later took on the role of CEO. At that time, we were still in the early R&D stages for our balloons. The result of that work was a design for autonomous, long-duration balloons that communicate with operators via satellite. In 2024, we introduced our first AI forecasting model, WeatherMesh, to ingest the data from the balloons and give them high-level instructions on where to fly next to fill in specific data gaps.  Each balloon has an antenna that enables it to communicate via satellite.             Christie Hemm Klok           A technician assembles the valve used to vent gas.              Christie Hemm Klok         The main envelope of a WindBorne balloon is made from a thin, transparent film just 20 micrometers thick‚Äîless than half the thickness of a human hair‚Äîand the whole assembly weighs less than 2 kilograms. Each balloon has a bag of sand used as ballast; the balloon can release sand to rise higher or vent gas to descend to a different wind current. Each balloon‚Äôs onboard autonomous system plots how to use the winds at different elevations to reach the locations specified by its WeatherMesh instructions.Our GSBs, which collect orders of magnitude more data than single-use dropsondes, make up Atlas, our global constellation. Today, our GSBs can fly for well over 50 days at altitudes ranging from ground level up to around 24 kilometers. Atlas, which typically has hundreds of balloons in the air at any time, collects more in situ data each day than does the U.S. National Weather Service.Following our time at Stanford, the WindBorne team built a business by scaling our Atlas constellation and providing weather data as a service. At first, the balloons‚Äô navigation was guided by results from a traditional numerical weather-prediction model that ran on a supercomputer. But running that model required hundreds of times as much computing power as AI weather models do. As our constellation proved capable of collecting vast amounts of data, we knew we needed to build a model that could not only efficiently direct our balloon constellation but also assimilate its massive datasets.The Limitations of Traditional Forecast MethodsCurrently, most weather forecasts rely on physics-based numerical weather prediction. In the United States, this job is handled by the federal government‚Äôs Global Forecast System (GFS), which ingests data from satellites, ground stations, radar systems, and a worldwide network of conventional weather balloons. It runs on a supercomputer four times a day, using a technique called data assimilation to produce forecasts that extend up to 16 days out. Data assimilation interprets new data alongside historical data to come up with the most accurate forecast possible.But therein lies the problem: Forecasting models are only as accurate as the data they are fed. So with 85 percent of the global atmosphere not being regularly probed, current forecasts are hamstrung by the sparseness of the datasets available to them. You‚Äôve probably seen a hurricane‚Äôs forecast cone shift dramatically from one day to the next. That volatility comes in part from the incomplete data driving these models. What‚Äôs more, physics-based models require enormous computing resources, which translate into high operational costs.By 2028, we believe the entire globe could be under near-continuous observation by Atlas. Over the last few years, AI models have disrupted weather forecasting, proving that they can generate faster, less costly, and more accurate predictions when compared with the prior gold standard of physics-based numerical weather models. When the Chinese company Huawei introduced its Pangu-Weather model in 2023, it served notice that AI forecasting could not only compete with physics-based models, but it could even outperform them. Other recent AI weather models include Google DeepMind‚Äôs GraphCast and AIFS from the European Centre for Medium-Range Weather Forecasts. But our system outperforms all of them, sometimes by a very large measure.While they continue to smash records, AI models (including ours) still make use of traditional physics-based models in several ways. For starters, all AI models are trained on historical weather data and predictions produced by conventional systems. Without them, the model would have to rely on raw, real-time observations for training data, without historical context.AI models also inherently lack an advanced understanding of physics, so traditional models provide a baseline to ensure that AI-generated predictions are physically plausible. This assistance is especially important during extreme weather events, when physics-based models can help AI models simulate rare conditions based on atmospheric principles.How We Built our AI Weather-Forecasting ModelWhen the WindBorne team set out to build the initial version of WeatherMesh, we had three main goals. First, it had to be inexpensive to run. Second, it needed to be at least as accurate as the top physics-based models. Third, it had to deliver forecasts with a high spatial resolution, providing fine-grained predictions on the scale of tens of kilometers.We decided to use an architecture based on what are called transformers‚Äîthe same technology that powers large language models like ChatGPT‚Äîbecause transformers can process huge datasets efficiently once they‚Äôre trained. This architecture includes what AI mavens refer to as an encoder-processor-decoder structure. The encoder transforms raw weather data‚Äîthings like temperature, wind, and pressure‚Äîinto a simpler compressed format known as latent space, where patterns are easier for the model to work with. The processor then runs calculations in this latent space to predict how the weather will change over time. To create longer-range forecasts, we simply run the processor step multiple times, with the output of the last prediction step serving as the input for the next. Finally, the decoder translates the results back into real-world weather variables.We trained our first weather model at our headquarters using a cluster of a few dozen Nvidia RTX 4090 graphics processing units (GPUs), which cost far less than relying on cloud-computing services to handle hundreds of terabytes of atmospheric data. Setting up our own machines paid off. The hardware set us back about $100,000, but had we run all our training experiments in the cloud instead, it easily would have cost four times as much.  Copper wires threaded through the plastic help control the gas-venting system.              Christie Hemm Klok           The balloon material is only 20 micrometers thick, and each balloon weighs less than 2 kilograms when fully assembled.             Christie Hemm Klok         The first version of WeatherMesh was smaller, faster, and cheaper to operate than the AI weather models created by tech giants. During training, it used about one-fifteenth the computing power of DeepMind‚Äôs GraphCast and one-tenth that of Huawei‚Äôs Pangu-Weather. Its small size makes its stellar performance all the more notable: It outperformed both those AI models and traditional physics-based models.The early accuracy gains of WeatherMesh can be attributed to our data-collection method. Our GSBs collect 30 to 50 times as much data as do conventional balloons, and we feed that data directly into WeatherMesh. We measured our model‚Äôs accuracy based on frequency of errors when compared with other physics- and AI-based models. In 2024, we beat both Huawei‚Äôs Pangu-Weather and DeepMind‚Äôs GraphCast to become the most accurate AI forecasting model in the world. At the time this article is being published, in October 2025, WeatherMesh retains the lead.Our initial version of the model took in data and output forecasts at 0.25-degree resolution (about 25 kilometers per grid cell) to match the resolution of ERA5, a widely used historical weather dataset. Today, WeatherMesh also includes a component that can provide forecasts for selected locations at a resolution of about 1 km.Most AI weather models train on historical datasets like ERA5, which organizes decades of atmospheric data into a consistent framework. But we also wanted WeatherMesh to run ‚Äúlive,‚Äù ingesting real-time balloon observations and up-to-date analyses from the U.S. and European agencies. That transition was challenging, because most AI models perform worse when they shift from carefully curated historical data to messy real-world feeds.To address this issue, we built specialized adapters based on a type of neural-network architecture known as U-Net, which excels at learning spatial features across different scales. These adapters translate real-time data into the same internal format used for WeatherMesh‚Äôs training data. In this way we preserved the benefits of training on ERA5 while still delivering accurate real-time forecasts.Building On Success With WeatherMesh-4Following the success of our initial WeatherMesh model, we released the second, third, and fourth versions of the model in rapid succession. WeatherMesh-4 predicts standard atmospheric variables at 25 vertical levels throughout the atmosphere. It also predicts a wide range of conditions at the surface, including temperature and dewpoint at 2 meters from the ground, wind speed at 10 meters and 100 meters, minimum and maximum temperatures, precipitation, solar radiation, and total cloud cover. It can produce a full forecast every 10 minutes based on the latest observations. In contrast, traditional weather models update every 6 hours.  Traditional weather balloons stay aloft for only a few hours and don‚Äôt go far from their launch sites.            Annie Mulligan/Houston Chronicle/Getty Images        We‚Äôve run extensive benchmarks to compare the latest version of WeatherMesh with other popular forecasting systems. We‚Äôve found that the model‚Äôs predictions for the Earth‚Äôs surface and atmosphere are up to 30 percent more accurate than those from the traditional model from the European Centre for Medium-Range Weather Forecasts, and also surpass results from DeepMind‚Äôs latest model, GenCast, on most evaluations.Building an end-to-end system means the entire pipeline must work in harmony. Our balloon constellation can‚Äôt afford to wait 12 hours for a new forecast; it needs near-constant refreshes to navigate the skies. Meanwhile, the AI model uses fresh atmospheric data from the balloons to improve the accuracy of its forecasts. Balancing these requirements forced us to get creative about how we moved the data and ran the model, but ultimately we produced a powerful system that‚Äôs fast and responsive.What‚Äôs Next for WindBorneIn the coming years, our goal is to expand our Atlas balloon constellation to about 10,000 GSBs flying at any time, launched from about 30 sites worldwide. To achieve that goal we‚Äôll need roughly 300 launches per day, or 9,000 per month. By 2028, we believe the entire globe could be under near-continuous observation by Atlas, from the remote Pacific to the polar ice caps. And we continue to test the boundaries of what is possible: WindBorne recently kept a balloon aloft for a record-breaking 104 days.We‚Äôre not aiming to make physics-based weather models obsolete. We see a future where AI and traditional methods operate side by side, each reinforcing the other. Governments, researchers, and corporations can lean on these improved forecasts to guide disaster preparedness, aviation, supply-chain logistics, and more. Our planet‚Äôs weather challenges are only going to intensify as the climate continues to change, and improved forecasts are key to helping us prepare.Looking back at Hurricane Milton, it still feels surreal that our balloons managed to ride into a storm of that scale. Yet that was the moment WindBorne proved that a new and agile system could deliver real value where legacy methods fall short. In a world where an extra 12 or 24 hours of warning can mean the difference between safety and devastation, end-to-end AI forecasting offers a revolution in how people can observe, predict, and protect themselves from the most powerful forces on Earth. 

</details>

<details><summary><strong><a href='https://netflixtechblog.com/behind-the-streams-real-time-recommendations-for-live-events-e027cb313f8f?source=rss----2615bd06b42e---4' target='_blank'>Behind the Streams: Real-Time Recommendations for Live Events Part 3</a></strong> ‚Äî <em>2025-10-21 00:53:29</em></summary>

By: Kris Range, Ankush Gulati, Jim Isaacs, Jennifer Shin, Jeremy Kelly, Jason¬†TuThis is part 3 in a series called ‚ÄúBehind the Streams‚Äù. Check out part 1 and part 2 to learn¬†more.Picture this: It‚Äôs seconds before the biggest fight night in Netflix history. Sixty-five million fans are waiting, devices in hand, hearts pounding. The countdown hits zero. What does it take to get everyone to the action on time, every time? At Netflix, we‚Äôre used to on-demand viewing where everyone chooses their own moment. But with live events, millions are eager to join in at once. Our job: make sure our members never miss a¬†beat.When Live events break streaming records ¬π ¬≤ ¬≥, our infrastructure faces the ultimate stress test. Here‚Äôs how we engineered a discovery experience for a global audience excited to see a knockout.Why are Live Events Different?Unlike Video on Demand (VOD), members want to catch live events as they happen. There‚Äôs something uniquely exciting about being part of the moment. That means we only have a brief window to recommend a Live event at just the right time. Too early, excitement fades; too late, the moment is missed. Every second¬†counts.To capture that excitement, we enhanced our recommendation delivery systems to serve real-time suggestions, providing members richer and more compelling signals to hit play in the moment when it matters most. The challenge? Sending dynamic, timely updates concurrently to over a hundred million devices worldwide without creating a thundering herd effect that would overwhelm our cloud services. Simply scaling up linearly isn‚Äôt efficient and reliable. For popular events, it could also divert resources from other critical services. We needed a smarter and more scalable solution than just adding more resources.Orchestrating the moment: Real-time RecommendationsWith millions of devices online and live event schedules that can shift in real time, the challenge was to keep everyone perfectly in sync. We set out to solve this by building a system that doesn‚Äôt just react, but adapts by dynamically updating recommendations as the event unfolds. We identified the need to balance three constraints:Time: the duration required to coordinate an¬†update.Request throughput: the capacity of our cloud services to handle requests.Compute cardinality: the variety of requests necessary to serve a unique¬†update.Visualizing constraints for real-time updatesWe solved this constraint optimization problem by splitting the real-time recommendations into two phases: prefetching and real-time broadcasting. First, we prefetch the necessary data ahead of time, distributing the load over a longer period to avoid traffic spikes. When the Live event starts or ends, we broadcast a low cardinality message to all connected devices, prompting them to use the prefetched data locally. The timing of the broadcast also adapts when event times shift to preserve accuracy with the production of the Live event. By combining these two phases, we‚Äôre able to keep our members‚Äô devices in sync and solve the thundering herd problem. To maximize device reach, especially for those with unstable networks, we use ‚Äúat least once‚Äù broadcasts to ensure every device gets the latest updates and can catch up on any previously missed broadcasts as soon as they‚Äôre back¬†online.The first phase optimizes request throughput and compute cardinality by prefetching materialized recommendations, displayed title metadata, and artwork for a Live event. As members naturally browse their devices before the event, this data is prepopulated and stored locally in device cache, awaiting the notification trigger to serve the recommendations instantaneously. By distributing these requests naturally over time ahead of the event, we can eliminate any related traffic spikes and avoid the need for large-scale, real-time system¬†scaling.A phased approach, smoothing traffic requests over time with a real-time low-cardinality broadcastThe second phase optimizes request throughput and time to update devices by broadcasting a low-cardinality, real-time message to all connected devices at critical moments in a Live event‚Äôs lifecycle. Each broadcast payload includes a state key and a timestamp. The state key indicates the current stage of the Live event, allowing devices to use their pre-fetched data to update cached responses locally without additional server requests. The timestamp ensures that if a device misses a broadcast due to network issues, it can catch up by replaying missed updates upon reconnecting. This mechanism guarantees devices receive updates at least once, significantly increasing delivery reliability even on unstable networks.A phased approach optimizes each constraint to ensure we can deliver for the big¬†moment!Moment in Numbers: During peak load, we have successfully delivered updates at multiple stages of our events to over 100 million devices in under a¬†minute.Under the Hood: How It¬†WorksWith the big picture in mind, let‚Äôs examine how these pieces interact in practice.In the diagram below, the Message Producer microservice centralizes all of the business logic. It continuously monitors live events for setup and timing changes. When it detects an update, it schedules broadcasts to be sent at precisely the right moment. The Message Producer also standardizes communication by providing a concise GraphQL schema for both device queries and broadcast payloads.Rather than sending broadcasts directly to devices via WebSocket, the Message Producer hands them off to the Message Router. The Message Router is part of a robust two-tier pub/sub architecture built on proven technologies like Pushy (our WebSocket proxy), Apache Kafka, and Netflix‚Äôs KV key-value store. The Message Router tracks subscriptions at the Pushy node granularity, while Pushy nodes map the subscriptions to individual connections, creating a low-latency fanout that minimizes compute and bandwidth requirements.Devices interface with our GraphQL Domain Graph Service (DGS). These schemas offer multiple query interfaces for prefetching, allowing devices to tailor their requests to the specific experience being presented. Each response adheres to a consistent API that resolves to a map of stage keys, enabling fast lookups and keeping business logic off the device. Our broadcast schema specifies WebSocket connection parameters, the current event stage, and the timestamp of the last broadcast message. When a device receives a broadcast, it injects the payload directly into its cache, triggering an immediate update and re-render of the interface.Balancing the Moment: Throughput ManagementIn addition to building the new technology to support real-time recommendations, we also evaluated our existing systems for potential traffic hotspots. Using high-watermark traffic projections for live events, we generated synthetic traffic to simulate game-day scenarios and observed how our online services handled these bursts. Through this process, several common patterns¬†emerged:Breaking the Cache SynchronyOur game-day simulations revealed that while our approach mitigated the immediate thundering herd risks driven by member traffic during the events, live events introduced unexpected mini thundering herds in our systems hours before and after the actual events. The surge of members joining just in time for these events led to concentrated cache expirations and recomputations, which created traffic spikes well outside the event window that we did not anticipate. This was not a problem for VOD content because the member traffic patterns are a lot smoother. We found that fixed TTLs caused cache expirations and refresh-traffic spikes to happen all at once. To address this, we added jitter to server and client cache expirations to spread out refreshes and smooth out traffic¬†spikes.Adaptive Traffic PrioritizationWhile our services already leverage traffic prioritization and partitioning based on factors such as request type and device type, live events introduced a distinct challenge. These events generated brief traffic bursts that were intensely spiky and placed significant strain on our systems. Through simulations, we recognized the need for an additional event-driven layer of traffic management.To tackle this, we improved our traffic sharding strategies by using event-based signals. This enabled us to route live event traffic to dedicated clusters with more aggressive scaling policies. We also added a dynamic traffic prioritization ruleset that activates whenever we see high requests per second (RPS) to ensure our systems can handle the surge smoothly. During these peaks, we aggressively deprioritize non-critical server-driven updates so that our systems can devote resources to the most time-sensitive computations. This approach ensures smooth performance and reliability when demand is at its¬†highest.Snapshot of non-critical traffic volume decline (in %) for a member-facing service during a live event‚Ää‚Äî‚Ääachieved via aggressive de-prioritizationLooking AheadWhen we set out to build a seamlessly scalable scheduled viewing experience, our goal was to create a dynamic and richer member experience for live content. Popular live events like the Crawford v. Canelo fight and the NFL Christmas games truly put our systems to the test. Along the way, we also uncovered valuable learnings that continue to shape our work. Our attempts to deprioritize traffic to other non-critical services caused unexpected call patterns and spikes in traffic elsewhere. Similarly, in hindsight, we also learned that the high traffic volume from popular events caused excessive non-essential logging and was putting unnecessary pressure on our ingestion pipelines.None of this work would have been possible without our stunning colleagues at Netflix who collaborated across multiple functions to architect, build, and test these approaches, ensuring members can easily access events at the right moment: UI Engineering, Cloud Gateway, Data Science & Engineering, Search and Discovery, Evidence Engineering, Member Experience Foundations, Content Promotion and Distribution, Operations and Reliability, Device Playback, Experience and Design and Product Management.As Netflix‚Äôs content offering expands to include new formats like live titles, free-to-air linear content, and games, we‚Äôre excited to build on what we‚Äôve accomplished and look ahead to even more possibilities. Our roadmap includes extending the capabilities we developed for scheduled live viewing to these emerging formats. We‚Äôre also focused on enhancing our engineering tooling for greater visibility into operations, message delivery, and error handling to help us continue to deliver the best possible experience for our¬†members.Join Us for What‚Äôs¬†NextWe‚Äôre just scratching the surface of what‚Äôs possible as we bring new live experiences to members around the world. If you are looking to solve interesting technical challenges in a unique culture, then apply for a role that captures your curiosity.Look out for future blog posts in our ‚ÄúBehind the Streams‚Äù series, where we‚Äôll explore the systems that ensure viewers can watch live streams once they manage to find and play¬†them.Behind the Streams: Real-Time Recommendations for Live Events Part 3 was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.

</details>

<details><summary><strong><a href='https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/' target='_blank'>A picture's worth a thousand (private) words: Hierarchical generation of coherent synthetic photo albums</a></strong> ‚Äî <em>2025-10-20 21:54:00</em></summary>

Generative AI

</details>

<details><summary><strong><a href='https://developer.nvidia.com/blog/build-an-ai-agent-to-analyze-it-tickets-with-nvidia-nemotron/' target='_blank'>Build an AI Agent to Analyze IT Tickets with NVIDIA Nemotron</a></strong> ‚Äî <em>2025-10-20 17:00:00</em></summary>

Modern organizations generate a massive volume of operational data through ticketing systems, incident reports, service requests, support escalations, and more....

</details>

<details><summary><strong><a href='https://developer.nvidia.com/blog/enabling-scalable-ai-driven-molecular-dynamics-simulations/' target='_blank'>Enabling Scalable AI-Driven Molecular Dynamics Simulations</a></strong> ‚Äî <em>2025-10-20 16:30:00</em></summary>

Molecular dynamics (MD) simulations are a powerful tool in computational chemistry and materials science, and they‚Äôre essential for studying chemical...

</details>

<details><summary><strong><a href='https://developer.nvidia.com/blog/scaling-large-moe-models-with-wide-expert-parallelism-on-nvl72-rack-scale-systems/' target='_blank'>Scaling Large MoE Models with Wide Expert Parallelism on NVL72 Rack Scale Systems</a></strong> ‚Äî <em>2025-10-20 16:00:00</em></summary>

Modern AI workloads have moved well beyond single-GPU inference serving. Model parallelism, which efficiently splits computation across many GPUs, is now the...

</details>

