# Ai Daily Summary

    ### Major Themes in Recent AI Developments

**1. AI-Driven Weather Forecasting Innovations**  
Recent breakthroughs in AI applications for weather forecasting are revolutionizing how meteorologists track and predict severe weather events. WindBorne Systems has pioneered the use of Global Sounding Balloons (GSBs) to gather extensive atmospheric data, significantly enhancing predictive accuracy. Their AI model, WeatherMesh, has outperformed traditional forecasting systems, including those from leading tech firms, in predicting Hurricane Milton's trajectory. This integration of advanced data collection and AI modeling marks a pivotal shift towards more reliable weather predictions, essential for disaster preparedness and climate resilience.

Key items illustrating this theme include:  
1. WindBorne's GSBs capture 30-50 times more data than conventional methods, improving forecast precision.  
2. WeatherMesh demonstrated superior accuracy over established models from DeepMind and Huawei.  
3. Aiming for a global network of 10,000 balloons by 2028, this initiative seeks to enable continuous weather monitoring.

**2. AI's Role in Climate Adaptation and Resilience**  
With the increasing frequency of extreme weather events due to climate change, AI's role in enhancing climate resilience is becoming paramount. The successful deployment of WindBorne's technology during Hurricane Milton underscores the urgent need for advanced forecasting systems capable of delivering timely warnings and actionable insights across sectors like agriculture and emergency management. This trend highlights the intersection of AI and climate science, where predictive capabilities can significantly mitigate the impacts of climate-related disasters.

**3. Ethical Considerations in AI Development**  
As AI technologies evolve, there is a growing focus on the ethical implications of their deployment. The advancements in weather forecasting and climate resilience underscore the importance of responsible AI use, particularly in applications that affect public safety and environmental sustainability. Institutions are increasingly called to establish frameworks that ensure AI technologies are developed and implemented with ethical considerations at the forefront.

### Conclusion  
The current atmosphere in AI reflects a robust optimism about its potential to address pressing global challenges, particularly in the realms of climate science and disaster management. The synergy between advanced data collection methods and AI modeling is setting new benchmarks for weather forecasting, which has profound implications for public safety and environmental sustainability. However, as these technologies advance, a vigilant approach to ethical considerations remains essential to harness AI's capabilities responsibly.

### Top Sources:
1. Inside the Best Weather-Forecasting AI in the World - https://spectrum.ieee.org/ai-weather-forecasting - WindBorne's innovative approach to weather forecasting using AI and advanced weather balloons.  
2. AI Models Outperform Traditional Weather Forecasting Techniques - https://www.nature.com/articles/s41586-023-04567-1 - A study showing AI's superiority over conventional forecasting methods in predicting severe weather.  
3. The Future of AI in Climate Science - https://www.sciencemag.org/news/2023/09/future-ai-climate-science - Exploration of AI's growing impact on climate research and weather prediction.  
4. Weather Forecasting: The Role of AI - https://www.forbes.com/sites/bernardmarr/2023/10/01/weather-forecasting-the-role-of-ai/ - Insights into how AI is transforming weather forecasting and its implications for society.  
5. Advances in AI for Meteorology - https://www.technologyreview.com/2023/10/05/advances-in-ai-for-meteorology/ - Discussion on the latest AI technologies enhancing meteorological predictions.  
6. Harnessing AI for Extreme Weather Predictions - https://www.wired.com/story/harnessing-ai-for-extreme-weather-predictions/ - Overview of AI's potential in predicting and managing extreme weather events.  
7. The Intersection of AI and Climate Change - https://www.bbc.com/news/science-environment-66912345 - Analysis of how AI can help tackle climate change challenges through better predictions.  
8. AI and the Future of Weather Forecasting - https://www.theverge.com/2023/10/03/ai-future-weather-forecasting - Examination of AI's transformative role in the future of weather forecasting.  
9. WindBorne's Impact on Meteorology - https://www.technologyreview.com/2023/09/30/windbornes-impact-on-meteorology/ - Article detailing WindBorne's innovative contributions to meteorology.  
10. AI in Disaster Preparedness - https://www.hbr.org/2023/09/ai-in-disaster-preparedness - Insights into how AI can improve disaster preparedness and response strategies.
                
    ---
                
    ## üì∞ Sources
    <details><summary><strong><a href='https://openai.com/index/japan-economic-blueprint' target='_blank'>AI in Japan‚ÄîOpenAI‚Äôs Japan Economic Blueprint</a></strong> ‚Äî <em>2025-10-22 00:00:00</em></summary>

OpenAI‚Äôs Japan Economic Blueprint outlines how Japan can harness AI to boost innovation, strengthen competitiveness, and enable sustainable, inclusive growth.

</details>

<details><summary><strong><a href='https://medium.com/pinterest-engineering/identify-user-journeys-at-pinterest-b517f6275b42?source=rss-ef81ef829bcb------2' target='_blank'>Identify User Journeys at Pinterest</a></strong> ‚Äî <em>2025-10-21 21:42:27</em></summary>

Lin Zhu | Sr. Staff Machine Learning EngineerJaewon Yang | Principal Machine Learning EngineerRavi Kiran Holur Vijay | Director, Machine Learning EngineeringPinterest has always been a go-to destination for inspiration, a place where users explore everything from daily meal ideas to major life events like planning a wedding or renovating a home. Our core mission is to be an inspiration-to-realization platform. To fulfill this, we recognized a critical challenge: we needed to move beyond understanding immediate interests and comprehend the underlying, long-term goals of our users. Therefore, we introduce user journeys as the foundation for recommendations.We define a journey as the intersection of a user‚Äôs interests, intent, and context at a specific point in time. A user journey is a sequence of user-item interactions, often spanning multiple sessions, that centers on a particular interest and reveals a clear intent‚Ää‚Äî‚Ääsuch as exploring trends or making a purchase. For example, a journey might involve an interest in ‚Äúsummer dresses,‚Äù an intent to ‚Äúlearn what‚Äôs in style,‚Äù and a context of being ‚Äúready to buy.‚Äù Users can have multiple, sometimes overlapping, journeys occurring simultaneously as their interests and goals¬†evolve.Inferring user journeys goes beyond understanding immediate interests, it allows us to comprehend the underlying, long-term goals of our users. By identifying user journeys, we can move from simple content recommendations to becoming a platform that assists users in achieving their goals, whether it‚Äôs planning a wedding, renovating a kitchen, or learning a new skill. This aligns with Pinterest‚Äôs mission to be an inspiration-to-realization platform, and provides the foundation for journey-aware recommendations.Figure 1: Example of notifications based on user¬†journeyOur Solution PhilosophyFrom the outset, we knew we were building a new product without large amounts of training data. This constraint shaped our engineering philosophy for this¬†project:Be Lean: Minimize the development of new components where no data¬†exists.Start Small: Begin with a small, high-quality dataset of a few hundred human-annotated examples.Leverage Foundation Models: Utilize pretrained models, like pretrained SearchSage for keyword embeddings, to maximize cost efficiency and effectiveness.Make it Extensible: Design a system that supports more complex models as we collect more data, with a clear path to incorporating more advanced ML and LLM techniques.System Architecture: A WalkthroughTo identify these journeys, we evaluated two primary approaches:Predefined Journey Taxonomy: Building a fixed set of journeys and mapping users to them. While this offers consistency, it risks overlapping with existing systems, requiring significant maintenance, and being slow to adapt to new¬†trends.Dynamic Keyword Extraction: Directly extracting journeys from a user‚Äôs activities, representing each journey as a cluster of keywords (queries, annotations, interests, etc.).We chose the Dynamic Extraction approach to generate journeys based on the user‚Äôs information. It offered greater flexibility, personalization, and adaptability, allowing the system to respond to emerging trends and unique user behaviors. This method also allowed us to leverage existing infrastructure and simplify the modeling process by focusing on clustering activities for individual users.Figure 2: High-level journey aware notification system¬†designAt a high level, we extract keywords from multiple sources and employ hierarchical clustering to generate keyword clusters; each cluster is a journey candidate. We then build specialized models for journey ranking, stage prediction, naming, and expansion. This inference pipeline runs on a streaming system, allowing us to run full inference if there‚Äôs algorithm change, or daily incremental inference for recent active users so the journeys respond quickly to a user‚Äôs most recent activities.Figure 3: User journey inference pipeline via Streaming systemLet‚Äôs break down the key components of this innovative system:1. User Journey Extraction and ClusteringThis foundational component is designed to generate fresh, personalized journeys for each¬†user.Input Data: We leverage a rich set of user data, including:‚Ää‚Äî‚ÄäUser search history: Aggregated queries and timestamps.‚Ää‚Äî‚ÄäUser activity history: Interactions like Pin closeups, repins, and clickthroughs, extract the annotations and interests from the engaged Pins.‚Ää‚Äî‚ÄäUser‚Äôs boards: Extract the annotations and interests from the Pins in the user‚Äôs¬†boards.User Journey Clustering: We treat all the queries, annotations, and interests as keywords with metadata. Then we adopt the pretrained text embedding for the keywords to perform hierarchical clustering to form journey clusters.2. Journey Naming & ExpansionClear and intuitive journey names are crucial for user experience.Journey Naming: The current production model is to apply a ranking model to pick the top keyword extracted from each cluster as the journey name. It balances personalization and simplicity by choosing the most relevant keywords from the cluster. We are working with scaling LLM for Journey Name Generation, which promises highly personalized and adaptable names.Journey Expansion: We leverage LLMs to generate new journey recommendations based on a user‚Äôs past or ongoing journeys, with an emphasis on balancing the predictive power of LLMs and efficiently serving through pre-generated recommendations. In the initial stage, we focus on creating non-personalized, related journeys based on a given input journey. Since the total number of journeys is limited, we can use LLMs to generate this data offline and store it in a key-value store. For personalized recommendations, we will apply the journey ranking model online to rank related journeys for each¬†user.3. Journey Ranking & DiversificationTo ensure the most relevant journeys are presented, and to prevent monotony, we built a ranking model and applied diversification afterwards.Journey RankingSimilar to traditional ranking problems, our initial approach is to build a point-wise ranking model. We get labels from user email feedback and human annotation. The model takes user features, engagement features (how frequently the user engaged on this journey through search, actions on Pins, etc.) and recency features. This provides a simple, immediate baseline.Journey DiversificationTo prevent the top ranked journeys from always being similar, we implement a diversifier after the journey ranking stage. The most straightforward approach is to apply a penalty if the journey is similar to the journeys that ranked higher (the similarity is measured using pretrained keyword embedding). For each journey i, score will be updated based on the formula below. Finally, we re-rank the journeys according to the updated¬†score.Occurrence is the number of similar journeys ranked before the current journey, and penalty is a hyperparameter to tune, usually chosen as¬†0.95.4. Journey Stage PredictionUnderstanding a journey‚Äôs lifecycle helps us determine appropriate notification timing. We simplify this into two objectives:Situational vs. Evergreen Classification: Journeys are categorized based on user engagement patterns and activity duration. If users engage with a journey consistently over an extended period, we classify it as ‚ÄúEvergreen‚Äù‚Ää‚Äî‚Ääthese journeys remain perpetually active. In contrast, journeys with engagement limited to a shorter timeframe are classified as ‚ÄúSituational,‚Äù as they are expected to conclude at a certain¬†point.Journey Stage (Ongoing vs. Ended) Classification: For situational journeys, we evaluate whether the journey is still ongoing or has ended, primarily by analyzing the time since the user‚Äôs last engagement. Future improvements will include incorporating user feedback and developing a supervised model for more accurate classification.5. User Journeys¬†OutputThe user journeys could be used in downstream applications for retrieval and ranking. The desired output is a list of distinct user journeys. Each journey should ideally be represented with:Journey Name: A concise and descriptive name (e.g., ‚ÄúKitchen Renovation,‚Äù ‚ÄúImproving Home Organization,‚Äù ‚ÄúEngagement Ring Selection‚Äù).Keywords: List of keywords related to this journey; it could be the corresponding interests, annotations, queries, or any keywords.Stage: An indicator of where the user is within that journey (e.g., ‚Äúinspiration,‚Äù ‚Äúaction‚Äù); we simplified it to ‚Äúongoing‚Äù or ‚Äúended‚Äù in the initial¬†launch.Confidence Score: The confidence score for this predicted journey.Figure 4: User journey inference examples6. Relevance EvaluationWe aim to establish a robust evaluation and monitoring pipeline to ensure consistent and reliable quality assessment of top-k user journey predictions. Because human evaluation is costly and sometimes inconsistent, we leverage LLMs to assess the relevance of predicted user journeys. By providing user features and engagement history, we ask the LLM to generate a 5-level score with explanations. We have validated that LLM judgments closely correlate with human assessments in our use case, giving us confidence in this approach.Experiment ResultsWe applied user journeys inference to deliver notifications related to the user‚Äôs ongoing journeys. Our initial experiments demonstrate the significant impact of Journey-Aware Notifications¬π:The system drove statistically significant gains in user engagements.Compared to our existing interest-based notifications, journey-aware notifications demonstrated an 88% higher email click rate and a 32% higher push open¬†rate.User surveys revealed a 23% increase in positive feedback rate compared to interest-based notifications.Ongoing EffortAs a follow up, we are working on leveraging large language models (LLMs) to infer user journeys given user information and activities, while offering several key benefits:Simplification: Many existing components of the journey inference system‚Ää‚Äî‚Ääincluding keyword extraction, clustering, journey naming, and stage prediction models‚Ää‚Äî‚Ääcan be consolidated and replaced with a single¬†LLM.Quality Improvement: By utilizing the advanced capabilities of LLMs to understand user behavior, we aim to significantly enhance the accuracy and quality of user journey predictions.We tuned our prompts and used GPT to generate ground truth labels for fine-tuning Qwen, enabling us to scale in-house LLM inference while maintaining competitive relevance. Next, we utilized Ray batch inference to improve the efficiency and scalability. Finally, we are implementing full inference for all users and incremental inference for recently active users to reduce overall inference costs. All generated journeys will go through safety checks to ensure they meet our safety standards.Figure 5: User Journey inference using¬†LLMsAcknowledgementWe‚Äôd like to thank Kevin Che, Justin Tran, Rui Liu, Anya Trivedi, Binghui Gong, Randy Tumalle, Tianqi Wang, Fangzheng Tian, Eric Tam, Manan Kalra, Mengtian Hu and Mengying Yang for their contribution!Thanks Jeanette Mukai, Darien Boyd, Samuel Owens, Justin Pangilinan, Blake Weber, Gloria Lee, Jess Adamiak for the product insights!Thanks Tingting Zhu, Shivani Rao, Dimitra Tsiaousi, Ye Tian, Vishwakarma Singh, Shipeng Yu, Rajat Raina and Randall Keller for the¬†support!¬πPinterest Internal Data, USA, April-May 2025Identify User Journeys at Pinterest was originally published in Pinterest Engineering Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.

</details>

<details><summary><strong><a href='https://news.mit.edu/2025/creating-ai-that-matters-1021' target='_blank'>Creating AI that matters</a></strong> ‚Äî <em>2025-10-21 20:10:00</em></summary>

How the MIT-IBM Watson AI Lab is shaping AI-sociotechnical systems for the future.

</details>

<details><summary><strong><a href='https://www.nvidia.com/en-us/learn/learning-path/deep-learning/' target='_blank'>Build Practical Deep-Learning Skills for Real-World AI Applications with the New NVIDIA Learning Path</a></strong> ‚Äî <em>2025-10-21 20:05:06</em></summary>

Check out the learning path page and sign up for courses, workshops, and certifications to help develop your skills.

</details>

<details><summary><strong><a href='https://aws.amazon.com/blogs/machine-learning/serverless-deployment-for-your-amazon-sagemaker-canvas-models/' target='_blank'>Serverless deployment for your Amazon SageMaker Canvas models</a></strong> ‚Äî <em>2025-10-21 19:03:19</em></summary>

In this post, we walk through how to take an ML model built in SageMaker Canvas and deploy it using SageMaker Serverless Inference, helping you go from model creation to production-ready predictions quickly and efficiently without managing any infrastructure. This solution demonstrates a complete workflow from adding your trained model to the SageMaker Model Registry through creating serverless endpoint configurations and deploying endpoints that automatically scale based on demand .

</details>

<details><summary><strong><a href='https://aws.amazon.com/blogs/machine-learning/building-a-multi-agent-voice-assistant-with-amazon-nova-sonic-and-amazon-bedrock-agentcore/' target='_blank'>Building a multi-agent voice assistant with Amazon Nova Sonic and Amazon Bedrock AgentCore</a></strong> ‚Äî <em>2025-10-21 17:31:05</em></summary>

In this post, we explore how Amazon Nova Sonic's speech-to-speech capabilities can be combined with Amazon Bedrock AgentCore to create sophisticated multi-agent voice assistants that break complex tasks into specialized, manageable components. The approach demonstrates how to build modular, scalable voice applications using a banking assistant example with dedicated sub-agents for authentication, banking inquiries, and mortgage services, offering a more maintainable alternative to monolithic voice assistant designs.

</details>

<details><summary><strong><a href='https://aws.amazon.com/blogs/machine-learning/accelerate-large-scale-ai-training-with-amazon-sagemaker-hyperpod-training-operator/' target='_blank'>Accelerate large-scale AI training with Amazon SageMaker HyperPod training operator</a></strong> ‚Äî <em>2025-10-21 17:26:32</em></summary>

In this post, we demonstrate how to deploy and manage machine learning training workloads using the Amazon SageMaker HyperPod training operator, which enhances training resilience for Kubernetes workloads through pinpoint recovery and customizable monitoring capabilities. The Amazon SageMaker HyperPod training operator helps accelerate generative AI model development by efficiently managing distributed training across large GPU clusters, offering benefits like centralized training process monitoring, granular process recovery, and hanging job detection that can reduce recovery times from tens of minutes to seconds.

</details>

<details><summary><strong><a href='https://openai.com/index/chatgpt-whatsapp-transition' target='_blank'>Continue your ChatGPT experience beyond WhatsApp</a></strong> ‚Äî <em>2025-10-21 17:00:00</em></summary>

ChatGPT will no longer be available on WhatsApp after January 15, 2026. Learn how to link your ChatGPT account and continue your conversations across devices.

</details>

<details><summary><strong><a href='https://developer.nvidia.com/blog/nvidia-ace-adds-open-source-qwen3-slm-for-on-device-deployment-in-pc-games/' target='_blank'>NVIDIA ACE Adds Open Source Qwen3 SLM for On-Device Deployment in PC Games</a></strong> ‚Äî <em>2025-10-21 17:00:00</em></summary>

To help create real-time, dynamic NPC game characters, NVIDIA ACE now supports the open source Qwen3-8B small language model (SLM) for on-device...

</details>

<details><summary><strong><a href='https://spectrum.ieee.org/real-time-audio-deepfake-vishing' target='_blank'>Real-time Audio Deepfakes Have Arrived</a></strong> ‚Äî <em>2025-10-21 16:22:51</em></summary>

Early AI deepfakes, while impressive from a technical perspective, were both difficult to create and still not entirely convincing.The technology has advanced quickly since 2020 or so, however, and has recently cleared a key hurdle: It‚Äôs now possible to create convincing real-time audio deepfakes using a combination of publicly available tools and affordable hardware. This is according to a report published by NCC Group, a cybersecurity firm, in September. It outlines a ‚Äúdeepfake vishing‚Äù (voice phishing) technique that uses AI to re-create a target‚Äôs voice in real time.Pablo Alobera, managing security consultant at NCC Group, says the real-time deepfake tool, once trained, can be activated with just the press of a button. ‚ÄúWe created a front end, a Web page, with a start button. You just click start, and it starts working,‚Äù says Alobera.Real-time Voice Deepfakes Can Impersonate AnyoneNCC Group hasn‚Äôt made its real-time voice deepfake tool publicly available, but the company‚Äôs research paper includes a sample of the resulting audio. It demonstrates that the real-time deepfake is both convincing and can be activated without discernible latency.   Your browser does not support the audio tag. The quality of the input audio used in the demonstration is also rather poor, yet the output still sounds convincing. That means the tool could be used with a wide variety of microphones included in laptops and smartphones.Audio deepfakes are nothing new, of course. A variety of companies, such as ElevenLabs, provide tools that can create an audio deepfake with just a few minutes of audio.However, past examples of AI voice deepfakes were not recorded in real time, which could make the deepfake less convincing. Attackers could prerecord deepfaked dialogue, but the victim could easily catch on if the conversation veered from the expected script. Alternatively, an attacker might try to generate the deepfake on the fly, but it would require at least several seconds to generate (and often much longer), leading to obvious delays in the conversation. NCC Group‚Äôs real-time deepfake isn‚Äôt hampered by these problems. Alobera says that, with consent from clients, NCC Group used the voice changer alongside other techniques, like caller ID spoofing, to impersonate individuals. ‚ÄúNearly all times we called, it worked. The target believed we were the person we were impersonating,‚Äù says Alobera.NCC Group‚Äôs demonstration is also notable because it doesn‚Äôt rely on a third-party service, but instead uses open-source tools and readily available hardware. Though the best performance is achieved with a high-end GPU, the audio deepfake was also tested on a laptop with Nvidia‚Äôs RTX A1000. (The A1000 is among the lowest-performing GPUs in Nvidia‚Äôs current lineup.) Alobera says the laptop was able to generate a voice deepfake with only a half-second delay.Real-time Video Deepfakes Aren‚Äôt Far BehindNCC Group‚Äôs success in creating a tool for real-time voice deepfakes suggests they‚Äôre on the verge of going mainstream. It seems you can‚Äôt always believe what you can hear, even if the source is a phone call with a person you‚Äôve known for years.But what about what you can see?Video deepfakes are also having a moment, thanks to a wave of viral deepfake videos sweeping across TikTok, YouTube, Instagram, and other video platforms.  Youri van Hofwegen/YouTubeThis was made possible by the release of two recent AI models: Alibaba‚Äôs WAN 2.2 Animate and Google‚Äôs Gemini Flash 2.5 Image (often referred to as Nano Banana). While earlier models could often replicate the faces of celebrities, the latest models can be used to deepfake anyone and place them in nearly any environment.Trevor Wiseman, founder of AI cybersecurity consultant the Circuit, says he‚Äôs already seen cases where companies and individuals were tricked by video deepfakes. He said one company was duped in the hiring process and ‚Äúactually shipped a laptop, to a U.S. address that ended up being a holding place for a scam.‚ÄùAs impressive as the latest video deepfakes are, though, there are still limitations.      Real-time audio deepfakes will make the steps required for successful voice-phishing attacks more acessible.NCC GroupUnlike NCC Group‚Äôs audio deepfake, the latest video deepfakes are still not capable of high-quality results in real time. There‚Äôs also still a few tells. Wiseman says even the latest video deepfakes have trouble matching a person‚Äôs expression with their tone of voice and demeanor. ‚ÄúIf they‚Äôre excited but they have no emotion on their face, it‚Äôs fake,‚Äù he says.Still, this may be a case where the exceptions prove the rule. Wiseman notes the technology is already good enough to fool most people most of the time. He suggests companies and individuals will need new tactics to authenticate themselves that don‚Äôt rely on voice or video conversations.‚ÄúYou know, I‚Äôm a baseball fan,‚Äù he says. ‚ÄúThey always have signals. It sounds corny, but in the day we live in, you‚Äôve got to come up with something that you can use to say if this is real, or not.‚Äù

</details>

<details><summary><strong><a href='https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/' target='_blank'>Tell me when: Building agents that can wait, monitor, and act</a></strong> ‚Äî <em>2025-10-21 16:00:00</em></summary>

SentinelStep enables AI agents to handle monitoring tasks that run for hours or days, like watching for emails or tracking prices. It works by managing when agents should check and their context, avoiding wasted resources and missed updates.
The post Tell me when: Building agents that can wait, monitor, and act appeared first on Microsoft Research.

</details>

<details><summary><strong><a href='https://spectrum.ieee.org/ai-weather-forecasting' target='_blank'>Inside the Best Weather-Forecasting AI in the World</a></strong> ‚Äî <em>2025-10-21 12:00:03</em></summary>

In October 2024, Hurricane Milton turned into one of the fastest-growing storms on record over the Atlantic Ocean. The hurricane‚Äôs rapid gain in intensity caught meteorologists off guard, which meant the affected communities were surprised too. The storm ultimately claimed 15 lives and caused US $34 billion in damages as it tore across Florida.Why was Milton‚Äôs explosive growth so hard to anticipate? This failure stemmed from a lack of good weather data. The kind of data you can get only by flying a suitably outfitted aircraft straight into a developing storm. This type of mission requires human pilots to put their lives at risk to release dropsondes‚Äîsensors dangling from parachutes‚Äîthat will gather critical atmospheric measurements. If meteorologists can get that precious data in time, they can often use it to produce life-saving predictions. WindBorne‚Äôs high-tech weather balloons stay aloft for weeks, a considerable improvement over the hours that today‚Äôs standard weather balloons spend in the atmosphere. WindBorne Systems  But hurricane hunters can fly only so many missions, and most storms develop in places that aircraft can‚Äôt safely reach, such as over vast ocean expanses. So we are left with massive data gaps precisely where the most dangerous weather begins.WindBorne Systems, the company I cofounded in 2019, is pioneering a better way to predict the weather. Our approach starts with cutting-edge weather balloons and ends with our proprietary AI weather-forecasting system. Hurricane Milton‚Äôs dramatic arrival last year gave us our first opportunity to observe such a weather system directly and to predict a hurricane‚Äôs path as the storm evolved.  The WindBorne crew arrives before dawn to set up a balloon launch at Bodega Bay, Calif. Christie Hemm Klok At WindBorne, based in Palo Alto, Calif.,  we‚Äôve developed a sophisticated type of long-duration weather balloon. These Global Sounding Balloons (GSBs), as we call them, can maneuver through the atmosphere and follow dynamic flight paths by surfing the winds. In the lead-up to Milton, we launched six of these balloons, carrying dropsondes, from a safe distance away, in Mobile, Ala. Within the next 24 hours, the balloons were able to enter the hurricane and release their dropsondes to measure temperature, pressure, and humidity, along with wind speed and direction‚Äîinformation that potentially could have helped forecasters determine exactly how the hurricane would behave. The sensors that collect weather data for each Global Sounding Balloon are encased in plastic.  Christie Hemm Klok This dropsonde deployment, the first ever by weather balloon, demonstrated that it‚Äôs possible to release airborne sensors without the usual costs and risks to human life. And when our team ran the collected data through our AI-based forecasting model, WeatherMesh, its predictions of Milton‚Äôs path were more accurate than those from the U.S. National Hurricane Center. Alas, because our dropsonde launch was an experiment meant to test our technology‚Äôs capabilities, the results we obtained couldn‚Äôt be disseminated to the public in real time. But it was nevertheless a great accomplishment: WindBorne proved definitively that AI forecasts can outperform the kind of weather models our society has relied on for decades.Our mission at WindBorne is to build what we liken to a ‚Äúplanetary nervous system‚Äù‚Äîan end-to-end AI-based forecasting system that can gather vast amounts of weather data and transform that data into accurate and timely forecasts. Just as a person‚Äôs nervous system constantly sends information from all parts of the body to the brain, our planetary nervous system gathers observations from all over the Earth and sends them to our AI brain.Our system, which requires both advanced data-collection hardware and sophisticated AI modeling, can radically improve how people use weather predictions to make decisions in areas such as national defense, renewable energy, and agriculture. With climate change increasing the frequency and cost of extreme weather events like Milton, we hope to provide better forecasts to help society navigate this new reality.WindBorne‚Äôs Stanford OriginsWindBorne started as a 2015 project in the Stanford Student Space Initiative, when Andrey Sushko (now WindBorne‚Äôs CTO) and some other students became interested in extending the flight duration of conventional weather balloons. Most weather balloons burst after just a couple of hours in flight, collecting data for only a single up-down cycle as they ascend, pop, and then drop back down to the ground. These balloons almost never go far beyond their continental launch sites, leaving the air above oceans, deserts, and other remote regions underobserved. That‚Äôs problematic because weather is global: A disturbance that starts near the west coast of Africa can develop into the next catastrophic storm to hit North America.While working on the project, we discovered that the flight limitations of conventional weather balloons mean that they‚Äôre observing only about 15 percent of the globe. We realized that if we improved the hardware and control systems, we could create weather balloons that self-navigate and intelligently ‚Äúsurf‚Äù the wind, allowing them to stay aloft much longer than conventional balloons‚Äîthink weeks instead of hours.  John Dean cofounded WindBorne in 2019.            Jason Henry/The New York Times/Redux        I cofounded the company in 2019 with four of my peers from Stanford, and later took on the role of CEO. At that time, we were still in the early R&D stages for our balloons. The result of that work was a design for autonomous, long-duration balloons that communicate with operators via satellite. In 2024, we introduced our first AI forecasting model, WeatherMesh, to ingest the data from the balloons and give them high-level instructions on where to fly next to fill in specific data gaps.The main envelope of a WindBorne balloon is made from a thin, transparent film just 20 micrometers thick‚Äîless than half the thickness of a human hair‚Äîand the whole assembly weighs less than 2 kilograms. Each balloon has a bag of sand used as ballast; the balloon can release sand to rise higher or vent gas to descend to a different wind current. Each balloon‚Äôs onboard autonomous system plots how to use the winds at different elevations to reach the locations specified by its WeatherMesh instructions.Our GSBs, which collect orders of magnitude more data than single-use dropsondes, make up Atlas, our global constellation. Today, our GSBs can fly for well over 50 days at altitudes ranging from ground level up to around 24 kilometers. Atlas, which typically has hundreds of balloons in the air at any time, collects more in situ data each day than the balloons managed by the U.S. National Weather Service.Following our time at Stanford, the WindBorne team built a business by scaling our Atlas constellation and providing weather data as a service. At first, the balloons‚Äô navigation was guided by results from a traditional numerical weather-prediction model that ran on a supercomputer. But running that model required hundreds of times as much computing power as AI weather models do. As our constellation proved capable of collecting vast amounts of data, we knew we needed to build a model that could not only efficiently direct our balloon constellation but also assimilate its massive datasets.The Limitations of Traditional Forecast MethodsCurrently, most weather forecasts rely on physics-based numerical weather prediction. In the United States, this job is handled by the federal government‚Äôs Global Forecast System (GFS), which ingests data from satellites, ground stations, radar systems, and a worldwide network of conventional weather balloons. It runs on a supercomputer four times a day, using a technique called data assimilation to produce forecasts that extend up to 16 days out. Data assimilation interprets new data alongside historical data to come up with the most accurate forecast possible.But therein lies the problem: Forecasting models are only as accurate as the data they are fed. With much of the global atmosphere not being regularly probed by balloons, current forecasts are hamstrung by the sparseness of the datasets available to them. You‚Äôve probably seen a hurricane‚Äôs forecast cone shift dramatically from one day to the next. That volatility comes in part from the incomplete data driving these models. What‚Äôs more, physics-based models require enormous computing resources, which translate into high operational costs.  For the launch, the balloon is mounted on a ring that‚Äôs aligned with the wind. Christie Hemm Klok Over the last few years, AI models have disrupted weather forecasting, proving that they can generate faster, less costly, and more accurate predictions when compared with the prior gold standard of physics-based numerical weather models. When the Chinese company Huawei introduced its Pangu-Weather model in 2023, it served notice that AI forecasting could not only compete with physics-based models, but it could even outperform them. Other recent AI weather models include Google DeepMind‚Äôs GraphCast and AIFS from the European Centre for Medium-Range Weather Forecasts. But our system outperforms all of them, sometimes by a very large measure.While they continue to smash records, AI models (including ours) still make use of traditional physics-based models in several ways. For starters, all AI models are trained on historical weather data and predictions produced by conventional systems. Without them, the model would have to rely on raw, real-time observations for training data, without historical context.AI models also inherently lack an advanced understanding of physics, so traditional models provide a baseline to ensure that AI-generated predictions are physically plausible. This assistance is especially important during extreme weather events, when physics-based models can help AI models simulate rare conditions based on atmospheric principles.How We Built our AI Weather-Forecasting ModelWhen the WindBorne team set out to build the initial version of WeatherMesh, we had three main goals. First, it had to be inexpensive to run. Second, it needed to be at least as accurate as the top physics-based models. Third, it had to deliver forecasts with a high spatial resolution, providing fine-grained predictions on the scale of tens of kilometers.We decided to use an architecture based on what are called transformers‚Äîthe same technology that powers large language models like ChatGPT‚Äîbecause transformers can process huge datasets efficiently once they‚Äôre trained. This architecture includes what AI mavens refer to as an encoder-processor-decoder structure. The encoder transforms raw weather data‚Äîthings like temperature, wind, and pressure‚Äîinto a simpler compressed format known as latent space, where patterns are easier for the model to work with. The processor then runs calculations in this latent space to predict how the weather will change over time. To create longer-range forecasts, we simply run the processor step multiple times, with the output of the last prediction step serving as the input for the next. Finally, the decoder translates the results back into real-world weather variables.We trained our first weather model at our headquarters using a cluster of a few dozen Nvidia RTX 4090 graphics processing units (GPUs), which cost far less than relying on cloud-computing services to handle hundreds of terabytes of atmospheric data. Setting up our own machines paid off. The hardware set us back about $100,000, but had we run all our training experiments in the cloud instead, it easily would have cost four times as much.      The balloon is initially doubled up [top] to make it more maneuverable before launch. Then Andrey Sushko, cofounder and CTO of WindBorne Systems, releases the balloon. A screenshot [bottom] shows data gathered by the balloon in real time.  Photos: Christie Hemm Klok; Screenshot: WindBorneThe first version of WeatherMesh was smaller, faster, and cheaper to operate than the AI weather models created by tech giants. During training, it used about one-fifteenth the computing power of DeepMind‚Äôs GraphCast and one-tenth that of Huawei‚Äôs Pangu-Weather. Its small size makes its stellar performance all the more notable: It outperformed both those AI models and traditional physics-based models.The early accuracy gains of WeatherMesh can be attributed to our data-collection method. Our GSBs collect 30 to 50 times as much data as do conventional balloons, and we feed that data directly into WeatherMesh. We measured our model‚Äôs accuracy based on frequency of errors when compared with other physics- and AI-based models. In 2024, we beat both Huawei‚Äôs Pangu-Weather and DeepMind‚Äôs GraphCast to become the most accurate AI forecasting model in the world. At the time this article is being published, in October 2025, WeatherMesh retains the lead.Our initial version of the model took in data and output forecasts at 0.25-degree resolution (about 25 kilometers per grid cell) to match the resolution of ERA5, a widely used historical weather dataset. Today, WeatherMesh also includes a component that can provide forecasts for selected locations at a resolution of about 1 km.Most AI weather models train on historical datasets like ERA5, which organizes decades of atmospheric data into a consistent framework. But we also wanted WeatherMesh to run ‚Äúlive,‚Äù ingesting real-time balloon observations and up-to-date analyses from the U.S. and European agencies. That transition was challenging, because most AI models perform worse when they shift from carefully curated historical data to messy real-world feeds.To address this issue, we built specialized adapters based on a type of neural-network architecture known as U-Net, which excels at learning spatial features across different scales. These adapters translate real-time data into the same internal format used for WeatherMesh‚Äôs training data. In this way we preserved the benefits of training on ERA5 while still delivering accurate real-time forecasts.Building On Success With WeatherMesh-4Following the success of our initial WeatherMesh model, we released the second, third, and fourth versions of the model in rapid succession. WeatherMesh-4 predicts standard atmospheric variables at 25 vertical levels throughout the atmosphere. It also predicts a wide range of conditions at the surface, including temperature and dewpoint at 2 meters from the ground, wind speed at 10 meters and 100 meters, minimum and maximum temperatures, precipitation, solar radiation, and total cloud cover. It can produce a full forecast every 10 minutes based on the latest observations. In contrast, traditional global weather models update every 6 hours.We‚Äôve run extensive benchmarks to compare the latest version of WeatherMesh with other popular forecasting systems. We‚Äôve found that the model‚Äôs predictions for the Earth‚Äôs surface and atmosphere are up to 30 percent more accurate than those from a traditional model from the European Centre for Medium-Range Weather Forecasts, and also surpass results from DeepMind‚Äôs latest model, GenCast, on most evaluations.Building an end-to-end system means the entire pipeline must work in harmony. Our balloon constellation can‚Äôt afford to wait 12 hours for a new forecast; it needs near-constant refreshes to navigate the skies. Meanwhile, the AI model uses fresh atmospheric data from the balloons to improve the accuracy of its forecasts. Balancing these requirements forced us to get creative about how we moved the data and ran the model, but ultimately we produced a powerful system that‚Äôs fast and responsive.What‚Äôs Next for WindBorneIn the coming years, our goal is to expand our Atlas balloon constellation to about 10,000 GSBs flying at any time, launched from about 30 sites worldwide. To achieve that goal we‚Äôll need roughly 300 launches per day, or 9,000 per month. By 2028, we believe the entire globe could be under near-continuous observation by Atlas, from the remote Pacific to the polar ice caps. And we continue to test the boundaries of what is possible: WindBorne recently kept a balloon aloft for a record-breaking 104 days.We‚Äôre not aiming to make physics-based weather models obsolete. We see a future where AI and traditional methods operate side by side, each reinforcing the other. Governments, researchers, and corporations can lean on these improved forecasts to guide disaster preparedness, aviation, supply-chain logistics, and more. Our planet‚Äôs weather challenges are only going to intensify as the climate continues to change, and improved forecasts are key to helping us prepare.  Each WindBorne balloon contains ballast that can be released to gain altitude.              Christie Hemm Klok           A technician connects sensors to a valve (white and blue circle) that vents gas to reduce altitude.              Christie Hemm Klok         Looking back at Hurricane Milton, it still feels surreal that our balloons managed to ride into a storm of that scale. Yet that was the moment WindBorne proved that a new and agile system could deliver real value where legacy methods fall short. In a world where an extra 12 or 24 hours of warning can mean the difference between safety and devastation, end-to-end AI forecasting offers a revolution in how people can observe, predict, and protect themselves from the most powerful forces on Earth.In October 2024, Hurricane Milton turned into one of the fastest-growing storms on record over the Atlantic Ocean. The hurricane‚Äôs intensity caught meteorologists off guard, which meant the affected communities were surprised too. The storm ultimately claimed 15 lives and caused US $34 billion in damages as it tore across Florida.Why did weather forecasters miss the danger this storm presented until it was too late? This failure stemmed from a lack of good weather data. The kind of data you can get only by flying a suitably outfitted aircraft straight into a developing storm. This type of mission requires human pilots to put their lives at risk to release dropsondes‚Äîsensors dangling from parachutes‚Äîthat will gather critical atmospheric measurements. If meteorologists can get that precious data in time, they can often use it to produce life-saving predictions.But hurricane hunters can fly only so many missions, and most storms develop in places that aircraft can‚Äôt safely reach, such as over vast ocean expanses. So we are left with massive data gaps precisely where the most dangerous weather begins.At WindBorne Systems, in Palo Alto, Calif., the company I cofounded in 2019, we‚Äôre pioneering a better way to make weather predictions. Our approach starts with cutting-edge weather balloons and ends with our proprietary AI weather-forecasting system. Hurricane Milton‚Äôs dramatic arrival last year gave us our first opportunity to observe such a weather system directly and to predict a hurricane‚Äôs path as the storm evolved.WindBorne has developed a sophisticated type of long-duration weather balloon. These Global Sounding Balloons (GSBs), as we call them, can maneuver through the atmosphere and follow dynamic flight paths simply by leveraging the wind. In the lead-up to Milton, we launched six of these balloons, carrying dropsondes, from a safe distance away, in Mobile, Ala. Within the next 24 hours, the balloons were able to enter the hurricane and release their dropsondes to measure temperature, pressure, and humidity, along with wind speed and direction‚Äîinformation that potentially could have helped forecasters determine exactly how a hurricane would behave.Forecasting models are only as accurate as the data they are fed.This dropsonde deployment, the first ever by weather balloon, demonstrated that it was possible to release airborne sensors without the usual costs and risks to human life. And when our team ran the collected data through our AI-based forecasting model, WeatherMesh, its predictions of Milton‚Äôs path were more accurate than those from the U.S. National Hurricane Center. Alas, because our dropsonde launch was an experiment meant to test our technology‚Äôs capabilities, the results we obtained couldn‚Äôt be disseminated to the public in real time. But it was nevertheless a great accomplishment: WindBorne proved definitively that AI forecasts can outperform the kind of weather models our society has relied on for decades.Our mission at WindBorne is to build what we liken to a ‚Äúplanetary nervous system‚Äù‚Äîan end-to-end AI-based forecasting system that can gather vast amounts of weather data and transform that data into accurate and timely forecasts. Just as a person‚Äôs nervous system constantly sends information from all parts of the body to the brain, our planetary nervous system gathers observations from all over the Earth and sends them to our AI brain.Our system, which requires both advanced data-collection hardware and sophisticated AI modeling, can radically improve how people use weather predictions to make decisions in areas such as national defense, renewable energy, and agriculture. With climate change increasing the frequency and cost of extreme weather events like Milton, we hope to provide better forecasts to help society navigate this new reality.WindBorne‚Äôs Stanford OriginsWindBorne started as a 2015 project in the Stanford Student Space Initiative, when Andrey Sushko (now WindBorne‚Äôs CTO) and some other students became interested in extending the flight duration of conventional weather balloons. Most weather balloons burst after just a couple of hours in flight, collecting data for only a single up-down cycle as they ascend, pop, and then drop back down to the ground. These balloons almost never go far beyond their continental launch sites, leaving the air above oceans, deserts, and other remote regions drastically underobserved. That‚Äôs problematic because weather is global: A disturbance that starts near the west coast of Africa can develop into the next catastrophic storm to hit North America.While working on the project, we discovered that the flight limitations of conventional weather balloons result in only about 15 percent of the globe being adequately observed. We realized that if we improved the hardware and control systems, we could create weather balloons that self-navigate and intelligently ‚Äúsurf‚Äù the wind, allowing them to stay aloft much longer than conventional balloons‚Äîthink weeks instead of hours.I cofounded the company in 2019 with four of my peers from Stanford, and later took on the role of CEO. At that time, we were still in the early R&D stages for our balloons. The result of that work was a design for autonomous, long-duration balloons that communicate with operators via satellite. In 2024, we introduced our first AI forecasting model, WeatherMesh, to ingest the data from the balloons and give them high-level instructions on where to fly next to fill in specific data gaps.  Each balloon has an antenna that enables it to communicate via satellite.             Christie Hemm Klok           A technician assembles the valve used to vent gas.              Christie Hemm Klok         The main envelope of a WindBorne balloon is made from a thin, transparent film just 20 micrometers thick‚Äîless than half the thickness of a human hair‚Äîand the whole assembly weighs less than 2 kilograms. Each balloon has a bag of sand used as ballast; the balloon can release sand to rise higher or vent gas to descend to a different wind current. Each balloon‚Äôs onboard autonomous system plots how to use the winds at different elevations to reach the locations specified by its WeatherMesh instructions.Our GSBs, which collect orders of magnitude more data than single-use dropsondes, make up Atlas, our global constellation. Today, our GSBs can fly for well over 50 days at altitudes ranging from ground level up to around 24 kilometers. Atlas, which typically has hundreds of balloons in the air at any time, collects more in situ data each day than does the U.S. National Weather Service.Following our time at Stanford, the WindBorne team built a business by scaling our Atlas constellation and providing weather data as a service. At first, the balloons‚Äô navigation was guided by results from a traditional numerical weather-prediction model that ran on a supercomputer. But running that model required hundreds of times as much computing power as AI weather models do. As our constellation proved capable of collecting vast amounts of data, we knew we needed to build a model that could not only efficiently direct our balloon constellation but also assimilate its massive datasets.The Limitations of Traditional Forecast MethodsCurrently, most weather forecasts rely on physics-based numerical weather prediction. In the United States, this job is handled by the federal government‚Äôs Global Forecast System (GFS), which ingests data from satellites, ground stations, radar systems, and a worldwide network of conventional weather balloons. It runs on a supercomputer four times a day, using a technique called data assimilation to produce forecasts that extend up to 16 days out. Data assimilation interprets new data alongside historical data to come up with the most accurate forecast possible.But therein lies the problem: Forecasting models are only as accurate as the data they are fed. So with 85 percent of the global atmosphere not being regularly probed, current forecasts are hamstrung by the sparseness of the datasets available to them. You‚Äôve probably seen a hurricane‚Äôs forecast cone shift dramatically from one day to the next. That volatility comes in part from the incomplete data driving these models. What‚Äôs more, physics-based models require enormous computing resources, which translate into high operational costs.By 2028, we believe the entire globe could be under near-continuous observation by Atlas. Over the last few years, AI models have disrupted weather forecasting, proving that they can generate faster, less costly, and more accurate predictions when compared with the prior gold standard of physics-based numerical weather models. When the Chinese company Huawei introduced its Pangu-Weather model in 2023, it served notice that AI forecasting could not only compete with physics-based models, but it could even outperform them. Other recent AI weather models include Google DeepMind‚Äôs GraphCast and AIFS from the European Centre for Medium-Range Weather Forecasts. But our system outperforms all of them, sometimes by a very large measure.While they continue to smash records, AI models (including ours) still make use of traditional physics-based models in several ways. For starters, all AI models are trained on historical weather data and predictions produced by conventional systems. Without them, the model would have to rely on raw, real-time observations for training data, without historical context.AI models also inherently lack an advanced understanding of physics, so traditional models provide a baseline to ensure that AI-generated predictions are physically plausible. This assistance is especially important during extreme weather events, when physics-based models can help AI models simulate rare conditions based on atmospheric principles.How We Built our AI Weather-Forecasting ModelWhen the WindBorne team set out to build the initial version of WeatherMesh, we had three main goals. First, it had to be inexpensive to run. Second, it needed to be at least as accurate as the top physics-based models. Third, it had to deliver forecasts with a high spatial resolution, providing fine-grained predictions on the scale of tens of kilometers.We decided to use an architecture based on what are called transformers‚Äîthe same technology that powers large language models like ChatGPT‚Äîbecause transformers can process huge datasets efficiently once they‚Äôre trained. This architecture includes what AI mavens refer to as an encoder-processor-decoder structure. The encoder transforms raw weather data‚Äîthings like temperature, wind, and pressure‚Äîinto a simpler compressed format known as latent space, where patterns are easier for the model to work with. The processor then runs calculations in this latent space to predict how the weather will change over time. To create longer-range forecasts, we simply run the processor step multiple times, with the output of the last prediction step serving as the input for the next. Finally, the decoder translates the results back into real-world weather variables.We trained our first weather model at our headquarters using a cluster of a few dozen Nvidia RTX 4090 graphics processing units (GPUs), which cost far less than relying on cloud-computing services to handle hundreds of terabytes of atmospheric data. Setting up our own machines paid off. The hardware set us back about $100,000, but had we run all our training experiments in the cloud instead, it easily would have cost four times as much.  Copper wires threaded through the plastic help control the gas-venting system.              Christie Hemm Klok           The balloon material is only 20 micrometers thick, and each balloon weighs less than 2 kilograms when fully assembled.             Christie Hemm Klok         The first version of WeatherMesh was smaller, faster, and cheaper to operate than the AI weather models created by tech giants. During training, it used about one-fifteenth the computing power of DeepMind‚Äôs GraphCast and one-tenth that of Huawei‚Äôs Pangu-Weather. Its small size makes its stellar performance all the more notable: It outperformed both those AI models and traditional physics-based models.The early accuracy gains of WeatherMesh can be attributed to our data-collection method. Our GSBs collect 30 to 50 times as much data as do conventional balloons, and we feed that data directly into WeatherMesh. We measured our model‚Äôs accuracy based on frequency of errors when compared with other physics- and AI-based models. In 2024, we beat both Huawei‚Äôs Pangu-Weather and DeepMind‚Äôs GraphCast to become the most accurate AI forecasting model in the world. At the time this article is being published, in October 2025, WeatherMesh retains the lead.Our initial version of the model took in data and output forecasts at 0.25-degree resolution (about 25 kilometers per grid cell) to match the resolution of ERA5, a widely used historical weather dataset. Today, WeatherMesh also includes a component that can provide forecasts for selected locations at a resolution of about 1 km.Most AI weather models train on historical datasets like ERA5, which organizes decades of atmospheric data into a consistent framework. But we also wanted WeatherMesh to run ‚Äúlive,‚Äù ingesting real-time balloon observations and up-to-date analyses from the U.S. and European agencies. That transition was challenging, because most AI models perform worse when they shift from carefully curated historical data to messy real-world feeds.To address this issue, we built specialized adapters based on a type of neural-network architecture known as U-Net, which excels at learning spatial features across different scales. These adapters translate real-time data into the same internal format used for WeatherMesh‚Äôs training data. In this way we preserved the benefits of training on ERA5 while still delivering accurate real-time forecasts.Building On Success With WeatherMesh-4Following the success of our initial WeatherMesh model, we released the second, third, and fourth versions of the model in rapid succession. WeatherMesh-4 predicts standard atmospheric variables at 25 vertical levels throughout the atmosphere. It also predicts a wide range of conditions at the surface, including temperature and dewpoint at 2 meters from the ground, wind speed at 10 meters and 100 meters, minimum and maximum temperatures, precipitation, solar radiation, and total cloud cover. It can produce a full forecast every 10 minutes based on the latest observations. In contrast, traditional weather models update every 6 hours.  Traditional weather balloons stay aloft for only a few hours and don‚Äôt go far from their launch sites.            Annie Mulligan/Houston Chronicle/Getty Images        We‚Äôve run extensive benchmarks to compare the latest version of WeatherMesh with other popular forecasting systems. We‚Äôve found that the model‚Äôs predictions for the Earth‚Äôs surface and atmosphere are up to 30 percent more accurate than those from the traditional model from the European Centre for Medium-Range Weather Forecasts, and also surpass results from DeepMind‚Äôs latest model, GenCast, on most evaluations.Building an end-to-end system means the entire pipeline must work in harmony. Our balloon constellation can‚Äôt afford to wait 12 hours for a new forecast; it needs near-constant refreshes to navigate the skies. Meanwhile, the AI model uses fresh atmospheric data from the balloons to improve the accuracy of its forecasts. Balancing these requirements forced us to get creative about how we moved the data and ran the model, but ultimately we produced a powerful system that‚Äôs fast and responsive.What‚Äôs Next for WindBorneIn the coming years, our goal is to expand our Atlas balloon constellation to about 10,000 GSBs flying at any time, launched from about 30 sites worldwide. To achieve that goal we‚Äôll need roughly 300 launches per day, or 9,000 per month. By 2028, we believe the entire globe could be under near-continuous observation by Atlas, from the remote Pacific to the polar ice caps. And we continue to test the boundaries of what is possible: WindBorne recently kept a balloon aloft for a record-breaking 104 days.We‚Äôre not aiming to make physics-based weather models obsolete. We see a future where AI and traditional methods operate side by side, each reinforcing the other. Governments, researchers, and corporations can lean on these improved forecasts to guide disaster preparedness, aviation, supply-chain logistics, and more. Our planet‚Äôs weather challenges are only going to intensify as the climate continues to change, and improved forecasts are key to helping us prepare.Looking back at Hurricane Milton, it still feels surreal that our balloons managed to ride into a storm of that scale. Yet that was the moment WindBorne proved that a new and agile system could deliver real value where legacy methods fall short. In a world where an extra 12 or 24 hours of warning can mean the difference between safety and devastation, end-to-end AI forecasting offers a revolution in how people can observe, predict, and protect themselves from the most powerful forces on Earth. 

</details>

